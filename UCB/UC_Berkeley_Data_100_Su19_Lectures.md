# UC Berkeley Data 100 Su19 Lectures
>
> <https://www.youtube.com/playlist?list=PLPHXc20GewP8J56CisONS_mFZWZAfa7jR>

## 1. Lecture 01 (Data Science Lifecycle, Study Design) - Data 100 Su19

### 数据爆炸

- 2010年,世界数据量约1.6 ZB(1 ZB=1万TB)
- 2013年增长到4.4 ZB
- 2019-2020年增长到44 ZB
- 数据主要存储在企业和组织的大量数据中心

### 数据用途

- 科学发现,如探测重力波、拍摄黑洞照片
- 工业优化,如联合包裹服务优化路线减少100亿英里行驶
- 搜索引擎如谷歌根据用户搜索行为提供结果

### 数据科学定义

- 应用数据集、计算式思维和信息思维来理解世界
- 需要计算、编程、统计学、领域专长等多方面技能
- 是一个交叉学科

### 本课概述

- 为后续数据科学课程做准备
- 重温探究数据采样概率知识
- 使用实际案例说明数据科学价值
- 让学生了解数据科学领域

## 2. Lecture 02 (pandas Part 1) - Data 100 Su19

### Pandas概览

Pandas是Python语言中处理结构化数据的主要库,它提供了数据结构和操作工具。

Pandas主要使用两个数据结构:

- 数据帧(DataFrame):表形式的数据集,数据以行和列组织,每个行和列都有标签索引。

- 系列(Series):一维数组,类似于列,也有标签索引。

数据帧和系列都拥有独特的索引,可以进行快速有效的选择、过滤和重排序。

### 数据帧

数据帧同表一样,由行和列构成,每个列可以是不同的数据类型。它支持快速和灵活的选择、过滤和整理数据。

数据帧的特征:

- 每一列有自己的列标签。

- 每一行也有自己的行标签,作为数据帧的索引。

- 可以通过行索引或列名称快速访问数据帧中的数据。

- 支持各种数据类型,允许混合不同类型的数据在同一数据结构中。

### 系列

系列是一维数组,可以理解为表中的一列。它也有自己的标签索引。

与数据帧不同,系列只包含单个数据列,没有行列的概念。但它也支持快速访问数据。

常见操作包括:提取数据帧中的某一列生成系列,处理后再放回数据帧。这给数据处理提供了许多便利。

### 总结

Pandas提供了数据帧和系列两个重要的数据结构。理解它们的基本概念和区别,为后续使用Pandas处理实际数据奠定了良好的基础。

## 4. Lecture 04 (Data 清洗) - Data 100 Su19

### 数据清洗

数据清洗就是将原始数据转化为可以用于分析的格式。这通常需要解决数据结构、编码、格式、缺失值等问题。

在现实世界中,数据收集的过程很混乱。因此当数据来到我们手中的时候,格式也很混乱。我们需要进行清洗来使数据更易于使用和分析。

### 矩形格式

数据清洗的目标是将数据转换成为我们称之为“矩形格式”的数据表格。

矩形格式意味着每个列中值的类型都是相同的,不会混合字符串和数字。这种格式下可以进行快速的排序、连接等操作。

panda数据框就是一个典型的矩形格式代表。每个行代表一个样本,每个列代表一个特征。

### 主键和外键

数据表中,不所有列都是平等的。通常会有一个或多个列作为主键。

主键唯一标识每一行。例如客户ID、产品ID等。

外键是表中引用其他表主键的列。例如订单表中的客户ID就是引用客户表中的主键。

主键和外键辨别对表连接分组等操作很重要。

### 数据问题

常见的数据问题包括:

- 缺失值替换为0
- 错别字
- 重复行
- 不一致的日期格式
- 复杂值提取特征等

这些问题都很难在收集阶段解决,需要后期迭代清洗才能找到并纠正。

清洗数据需要反复探索数据,找到问题,分类解决。这是个迭代的过程。

人工录入数据容易出错,是引起数据问题的重要原因。需要结合数据来源选择清洗策略。如果难以纠正,也可能放弃使用某些数据。

清洗后的数据需要检查以确保问题得到妥善解决。

## 5. Lecture 05 (Visualization using seaborn) - Data 100 Su19

### 一. 什么是可视化

可视化是将计算机可以读取的数值数据,像长列表中的数字或字符串等,转换成人类可以理解和看懂的图像形式。它通过为每个数据点创建一个标记,并将这个标记映射到一个视觉形式上来完成转换。

常见的可视化包括:

- 折线图:每个数据点使用一条线作为标记,并根据数值映射到y轴。
- 点图:每个数据点使用一个点作为标记。
- 散点图:每个数据点使用一个点作为标记,并根据两个变量映射到x轴和y轴。

可视化的最终目的是帮助人类判断数据模式并得出结论,而不仅仅是单纯传递数据。

### 二. 为什么需要可视化

1. 可视化可以让人类更快地发现数据模式。相比阅读长列表,看图形更容易发现规律。

2. 可视化可以同时展示大量数据。例如通过散点图可以一次看四组同分布不同形态的数据。

3. 可视化结果可以高效地传达信息。一个好的图表可以快速传达比模型或文字更多信息。

4. 人眼在视觉信息处理方面很强。可视化就是利用这一优势,利用视觉来理解数据。

5. 根据工作需要,掌握一定可视化技能即可成为数据分析员。这是种需求很高的技能。

### 三. Seaborn概述

Seaborn是一个基于Matplotlib的高级可视化库。它可以让我们更方便地生成美观的统计图表。

导入Seaborn需要同时导入Matplotlib,因为Seaborn内部调用了Matplotlib的很多函数。

Seaborn提供了丰富的函数来生成各种类型的统计图表,比如散点图、线图、密度图等。生成图表代码更简洁。

此外,Seaborn内置了美化设置,能自动应用合适的颜色和样式,使得图表看起来更直观美观。

### 四. Seaborn常见图表

- 散点图:描绘两个变量之间的关系
- 直方图:描述单变量的分布
- 密度图:展示单变量或两变量的连续概率分布  
- 折线图:描绘时序数据的变化趋势
- 箱线图:描述数据的分布及异常值
- 对角线图:展示数个变量之间的相关性

这些图表采用Seaborn生成代码更简洁,且自动应用了美观的主题设定。

人: 补充一下,里面没有提到如何使用Seaborn生成图表,可以补充一下基本使用方法:

## 6. Lecture 06 (Visualization Principles) - Data 100 Su19

### 人家产权组织与怀孕计划诉讼案例

2015年,美国政府组织怀孕计划被指控非法出售胎儿组织以获利。国会议员Chaffetz展示了一张图表,纵向为检查项目数量,横向为截止时间。图表仅有4个数据点,但无Y轴标注,给人误导之感。其目的是暗示怀孕计划近年主要做流产,不再做有价值的癌症检查,认为应该切断其资金。

### BLS劳工统计调查数据可视化

使用BLS网站数据,制作男女不同教育程度的周收入中值图表。图表轻易可比较不同教育程度的收入高低差异,但不易看出在同一教育程度下,男女平均工资差异。此差异比较重要。

### 华盛顿樱花10英里跑道赛结果图表

华盛顿每年四月有一场10英里路跑赛事。赛事近10年游走人数都在1.7万人以上。讲师提出如何设计一个更好的图表来展示参赛人数等多个维度的数据,以及其他数据可视化设计原则。

## 7. Lecture 07 (EDA, Text) - Data 100 Su19

### 数据类型

统计数据类型主要分为三类:

1. 名义型或分类型数据:没有明确顺序的类别,如政党阵营。

2. 有序型数据:具有自然顺序的类别,如衣服尺码或教育程度。

3. 数字型数据:代表数量或量的连续数值,如身高、天气温度等。

数字型数据的一个重要特征是,数据间的差异对分析结果有意义,能进行数学运算。

需注意计算机识别的数据类型与统计分析的数据类型的区分。例如性别编码为0/1/2时,计算机识别为数字,但实际上是名义型数据。

### 特征

探索性数据分析(EDA)的主要特征有:

1. 数据类型:名义型、有序型、数字型。

2. 粒度:数据整体/分组水平。

3. 范围:时间、地点等维度。

4. 时效性:静态数据或随时间变化。

5. 真实性:数据是否真实反映研究对象。

EDA的目的是找出预期和非预期的模式,为后续分析提供参考。分析结果不宜直接用于推断,应使用独立的数据集验证。

### EDA步骤

1. 视觉化数据,观察模式和问题。

2. 转换和筛选数据。

3. 探索发现假设,并用独立数据集验证。

4. 迭代进行上述步骤,不断进一步了解数据。

## 9. Lecture 09 (降维) - Data 100 Su19

### 维度

数据表中的列代表属性,但有些属性可能是冗余或相关的,不能真正提供额外信息。例如一个包含年龄和身高(英尺和英寸)三列的表,身高(英尺)可以由身高(英寸)计算得出,则表的维度为2。

维度用来描述数据表中的最小未重复属性数,会低于实际属性数。

### 高维数据

在生物测序、图像等领域的数据常常具有高维特性,例如人类基因组DNA序列包含约7000万碱基。直接可视化高维数据难度很大。

### 降维

目的在于将高维数据投影到低维空间,使数据分布的主要模式和结构保留下来,同时减少冗余信息。常用的方法包括:

1. 选择方差最大的两个属性投影。

2. 主成份分析(PCA),通过线性组合原属性形成新特征,以最大限度保留数据方差。

降维后可以进行数据可视化,发现数据间模式和聚类关系。但降维同时也会丢失一定信息。

正如本次视频中分享的例子,可以通过 congress 表征全国议员的投票数据,来描述议员的政党倾向关系,同时发现二维投影中的主要聚类模式。

## 10. Lecture 11 (PCA, Midterm Review) - Data 100 Su19

### PCA概述

主成分分析(PCA)是一种线性转换算法,用于消除多重共线性,并将高维数据集投影到低维空间。它通过奇异值分解来计算出主成分。

主成分代表了原始数据中的主要变化模式。前几个主成分通常可以解释数据集中的大部分变化。通过选择前几个主成分,可以将高维数据集映射到较低维的空间中。

### SVD与PCA联系

奇异值分解(SVD)将矩阵M分解为U∑V^T的形式,其中U和V都是正交矩阵,∑是对角矩阵。

对数据矩阵X进行SVD,可以得到U,∑,V。U矩阵的第i列代表第i主成分,∑的对角元素代表主成分的方差贡献率。通过选择前几列U和对应的∑元素,就可以压缩原始数据空间。

PCA的目的是找到一个低维子空间来最好地重建原始数据。第一主成分对应最大方差,第二主成分对应第二大方差,以此类推。选择前几个主成分就可以最大限度地保留原始数据中的变化信息。

### 计算与解释PCA结果

进行PCA分析主要步骤为:1)标准化数据;2)计算数据协方差矩阵;3)对协方差矩阵求特征值和特征向量;4)选择特征值最大的特征向量作为主成分。

主成分解释了原始数据中的多少方差。通过观察特征值能够判断主成分的重要性。累积贡献率随主成分数目增加而增加,选择贡献率达到指定值(如90%)的主成分个数。

### Midterm复习

中期考试时间定在下周二,考试地点在这个课堂。允许携带一张手写笔记作为参考。考试范围包括:研究设计,Pandas,可视化,字符串处理,SQL和PCA等主题。但不会考较难的线性代数知识。

## 11. Lecture 12 (Midterm Review) - Data 100 Su19

### PCA理论

主成分分析(PCA)是一种用于寻找数据集中最重要特征的技术。PCA的主要目的是:

1. 最大化方差。希望用尽可能少的主成分表达尽可能多的总体数据方差。

2. 从高维至低维地降维。能够将高维数据集分解为低维数据集,并用较少的主成分接近原始数据矩阵。

PCA适用于:

- 不确定数据中的趋势规律
- 认为数据秩较低,即少数特征决定数据集大部分信息
- 希望在低维空间观察高维数据集的簇状分布

### PCA应用时机

矩阵的秩定义为:用多少列可以表达整个矩阵。以线性组合方式计算。

例如速度-距离-时间三维矩阵,秩为3:

- 速度需要距离和时间共同决定,不能用它们的线性组合表示
- 但距离可以用时间的倒数表示,不增加秩
- 时间也可以用常数标量表示,不增加秩

所以秩为描述数据所需的独立特征数量。

### PCA算法原理

PCA选择variance最大的组成分,因为它们代表了数据集中的主要变化趋势。

但不是简单选择两个变化最大的特征,因为它们可能相关,来源于同一分布。PCA希望选择变异无关的特征组合,表达更全面信息。

## 14. Lecture 16 (Gradient Descent) - Data 100 Su19

### 一、导数与偏导数

- 导数表示函数在某一点的斜率,即如果我们改变自变量的值,则函数值会如何改变。

- 偏导数适用于多元函数,我们假设其他变量为常数,仅考虑某一变量,从而将多元函数简化为单变量函数,然后计算导数。

### 二、梯度

- 梯度是导数的多元版本,用于描述包含多个参数的函数在各个参数方向上的变化程度。

- 如果函数f依赖于θ这个参数向量,则梯度定义为:

梯度∇f(θ) = [ ∂f/∂θ1, ∂f/∂θ2, ..., ∂f/∂θn]^T

其中每一个元素为相应变量θ的偏导数。

- 梯度可以解释为函数在任意点的切面,我们可以将梯度插值到函数中得到一个切面拟合。

### 三、梯度下降

- 许多模型都是通过最小化损失函数来得到的,但对于复杂函数直接求解可能很难或不可行。

- 梯度下降是一种迭代算法,通过不断沿梯度的反方向调整参数θ,从而使损失函数逼近极小点,逐渐求解模型。

- 每次迭代都会对参数进行一个小步长的更新:

θ := θ - α*∇f(θ)

α为学习率,控制步长的大小。

- 通过重复此过程,可以逼近参数θ^*使得损失函数最小。梯度下降广泛应用于深度学习等场合拟合模型。

## 17. Lecture 19 (Logistic 回归) - Data 100 Su19

分类问题是预测分类型变量的一类预测问题。在分类问题中,我们建立模型来预测目标变量的类别。目标变量取值范围有限,二分类问题中变量只可能取0或1两个值。

将分类问题看作概率预测问题,可以将它变形为回归问题。分类问题中,我们预测目标类别的概率,而概率是一个连续值,所以可以看作回归问题来解决。

假设有一组训练样本集(x,y),x代表特征,y代表类别。我们想建立模型预测未知样本的类别概率P(Y=1|X)。这里的P(Y=1|X)代表给定特征X时,目标变量Y取值为1的概率。它实际上就是概率分布函数f*(X)。

logistic回归假设P(Y=1|X)随X变化呈с形函数关系。它通过学习特征X和类别Y之间的关系,估计出概率分布函数f*(X)。对新数据,通过 plug-in 概率分布函数f*(X)来预测观测值Y的概率。

logistic回归中的loss函数选择交叉熵(cross entropy)。它可以用来衡量预测概率与真实概率之间的差异。交叉熵会惩罚概率预测过于边缘(0或1)的情况。利用梯度下降来最小化交叉熵,求解回归系数θ。

logistic回归与线性回归很相似,但目标变量是类别型,所以采用sigmoid函数来限定预测概率在0-1区间内变化。它是二分类问题中广泛使用的分类算法。

## 19. Lecture 21 (决策边界,模型考虑) - Data 100 Su19

### 决策边界

决策边界是分类器预测从0到1的点集合。通常我们使用两维数据来绘图,把不同类别数据点用不同颜色区分开来。

Logistic回归会产生线性决策边界。我们可以添加多项式特征来产生非线性决策边界。比如添加二次项特征后,决策边界就会变成曲线形状。

随着多项式项数的增加,决策边界会变得更为复杂,容易过拟合。我们可以增加正则项来简化决策边界,达到去过拟合的目的。

K近邻算法的决策边界也很复杂。当增加近邻数目时,决策边界会变得更加平滑,模型偏置增加而方差减小。

### 模型优化方法

特征标准化可以帮助Logistic回归收敛,特别是加入正则化后。它通过让每个特征均值为0变量为1来消除特征差异导致的偏差。

Pipeline可以组合预处理和模型 fitting 步骤。我们可以从Pipeline中提取特定步骤的属性,比如获取Logistic回归模型的系数。

当数据维度高时,我们无法直接可视化多于两维的决策边界。这时可以使用PCA降维后再绘图。

只使用原始特征的时候,Logistic回归决策边界始终为线性。这是因为 Logistic回归实际上是一个近似线性的模型,尽管激活函数sigmoid非线性,但最终依然拟合出线性决策边界。

## 20. Lecture 22 (推断建模) - Data 100 Su19

### 一、推断的目的是利用样本来估计总体的参数,并得出总体的结论

在之前的例子中,我们从一个样本中计算出样本的比例为0.51。由于这个估计量是未偏的,我们可以得出结论称总体参数接近我们样本的比例。这就是推断的一个核心结果,即即使只有样本,我们也可以对总体作出一些结论。

但是,这个估计0.51几乎一定是错误的。因为我们的样本不是总体,真实的总体参数很可能是0.5、0.49甚至0.52等的值。

### 二、置信区间可以说明我们对总体参数的不确定性

为了表示我们对总体参数的不确定度,我们不仅给出一个值,而是一个值的范围。例如我们认为总体参数可能在0.48到0.53范围内。这个范围就叫做置信区间。

如果估计量波动较大,我们对估计结果的确定性就低,置信区间应该更宽一些。反之,如果估计量波动较小,我们对估计结果更有把握,置信区间可以更窄。

### 三、随机子采样法(bootstrap)可以无需多次回到总体就估计置信区间

如果我们能多次取样计算各个样本的均值,并看这些均值的分布,就可以画出置信区间。但是这意味着需要多次回到总体采样,在实际操作中效率很低。

随机子采样法的思想是:如果我们的样本与总体差异不大,我们可以视样本为“小总体”,从中多次随机取样构成“伪样本”,计算这些伪样本的均值分布,以此来模拟我们无法直接获得的真实总体均值分布,从而估计置信区间。

### 四、随着样本量的增加,估计量的波动越来越小,置信区间也会收缩

通过模拟示例,我们可以看到:样本均值呈正态分布,随着样本量的增加,波动范围减小;当样本量很大时,均值分布会接近于一个很窄的高峰。这时我们对总体参数的预测会更准确,置信区间也会更小。

## 21. Lecture 23 (Big Data and Ray) - Data 100 Su19

### 机器学习训练

机器学习训练指的是拟合机器学习模型到一些数据。经典范例有分类问题和回归问题。

分类问题是将数据点分到一个类别或另一个类别,比如图像是否包含狗、明天是否下雨等。回归问题是找出一条曲线最佳匹配数据点,比如预测房价。

这些问题使用随机梯度下降算法来求解。随机梯度下降是一种迭代式算法,每次随机取一小部分数据点来微调模型,逐步提高模型的预测能力。

虽然这种简单的分类与回归图形看起来很简单,但实际问题往往会更复杂,比如输入为图像向量,输出为图像中的车辆数量。此时使用高维复杂函数如神经网络来拟合,仍然可以使用随机梯度下降算法。

### 强化学习

强化学习中,Agent不仅要单次预测,还要做一系列决策来达成某个目标,比如游戏、决策控制等。不同于监督学习直接预测标签,强化学习通过试错来学习最优策略。

### 模型服务

训练好模型后,需要将其应用于实际服务中,比如翻译、语音识别等。这需要大量并发查询模型来响应用户请求,这就是模型服务。

### 超参数调优

机器学习实验中还需要调整许多超参数,比如模型类型、学习率等会影响结果。超参数调优是搜寻最佳超参数组合的过程。

## 22. Lecture 24 (Spark, Decision Trees)- Data 100 Su19

### spark

Spark可以将任务分配给多个核心进行并行计算。它支持map和reduce操作以及许多其他操作。

可以使用Spark分析大数据量,即使数据无法放在单个机器上。Spark支持使用DataFrame和SQL接口操作数据。

Spark可以缓存中间结果,第二次运行时会更快,因为不需要重新计算中间结果。

### 决策树

决策树通过一系列二值规则来表示模型,而不是线性回归中的系数。

树的每个节点代表对一个特征的判断,如“周末”或“工作日”。根据判断结果,决定跟下一步应该判断哪个特征。

通过这个过程可以将特征空间分割成一些矩形区域,每个区域都有一个分类结果。这样得到的规则可以解释模型的预测原因。

决策树可以用于分类和回归问题。回归树中节点可以直接预测一个数值,而不是分类。

决策树算法会自动从数据中学习这些规则。它通过递归地选择最优特征和最优分裂点构建树结构。

## 随机森林

随机森林通过构建多个决策树来提升模型性能,它通过随机取样训练子集和随机选择特征来实现模型的多样性。这可以有效减少过拟合。

随机森林属于集成学习方法,通过平均或多数表决的方式融合多个基学习器的预测结果,通常可以比单一决策树取得更好的效果。

## 23. Lecture 24 (决策树) - 数据100 Su19

在公司内,每个团队都有自己的数据库来记录日常运营数据。这些数据库称为操作数据存储,它们是当当时使用的实时数据库。

为了数据分析,公司会建立一个数据仓库。数据仓库通过ETL过程将各个团队数据库的数据抽取、转换、加载到一个集中的数据库中。这样就可以对历史数据进行统一查询分析,而不影响日常运营。

数据仓库内的数据通常会按星型模式存储。即将事实数据表作为中间,关联各种维度表。比如商品表、时间表等。这样可以避免冗余,更容易维护。

公司内的数据分析人员会利用SQL或者工具对数据仓库中的数据进行OLAP分析。比如做数据探索,为管理层制作报表和词图等决策支持。这就是业务智能。

除了结构化的数据外,公司还会收集大量非结构化的数据,如文本、图片视频等。这类数据难以直接录入到表格中进行分析。随着大数据时代来临,公司开始利用机器学习等技术对这类数据进行处理分析,以获得更全面的决策支持。

## 24. 第25次讲座(随机森林,运行时间分析,建模总结)-数据100 Su19

### 随机森林

上次我们介绍了决策树,但是单个决策树过拟合数据的问题。随机森林是一种通过建立多个决策树并相互平均的方法来解决这个问题。

随机森林的工作过程是:

1. 对数据进行随机重采样,每个重采样数据构建一个决策树;

2. 在构建每个决策树时,只使用数据集的一部分特征来进行节点分割;

3. 构建好的所有决策树进行平均,其预测结果就是随机森林的最终预测。

通过这种方法,每个决策树都会以不同的方式过拟合数据,其平均效果可以降低模型方差,避免过拟合问题。

随机森林在许多实际问题中表现很好,是目前常用的一种有效且健壮的分类与回归算法。

### 运行时间分析

软件系统的运行时间是一个重要的性能指标。我们可以对算法进行时间复杂度分析,帮助理解算法的计算效率,以及在大规模数据下的表现。

对不同算法进行时间复杂度分析,可以帮助我们选择在特定问题下效率更高的算法。但在实际应用中,还需要考虑算法的其他性质,如稳定性、 interpretability 等因素。

### 建模总结

这门课介绍了数据科学中很多基础但重要的内容,包括数据处理、特征工程、 supervised learning、model evaluation 和模型调优等各个步骤。

使用逻辑回归、决策树、集成学习等算法进行手把手的建模练习,可以帮助我们更深入地理解这些算法在理论和实践中的应用。

已经掌握的数据分析技能可以用于许多行业中的决策支持工作,为企业、政府和研究机构提供数值化的决策支持。同时也需要注意模型在实际应用中的一些限制和隐含假设。

数据科学是一个持续发展的领域。今后需要在实践中不断深入学习新的算法和技术,以应对不断变化的数据环境。

## 25. Lecture 26 (Ethics & Conclusion) - Data 100 Su19

### 数据科学与伦理

在进行数据科学研究时,需要思考自己的项目是否会带来不良后果。作为数据科学家,我们应该考虑项目可能影响的各种群体,并确保自己所采取的措施不会造成伤害。

在检验假设时,需要注意自己潜在的偏见,避免结果受到主观影响。可以与其他领域的专家共同工作,以补充自己在某些方面的知识短板。

### 实际案例研究流程

1. 确定研究问题:如是否Twitter上的讨论健康成熟?

2. 收集数据:使用 tweets 数据集。

3. 寻找目标量度:如情绪极端程度、互动程度等。

4. 使用技术工具分析数据:如情绪分类算法判断 tweets 极性。

5. 验证假设:比如高度互动代表成熟健康的讨论平台。

6. 总结结果:给出Twitter是否需要调整政策的建议。

7. 反思与改进:是否考虑到所有影响因素,是否需要额外专业知识支持等。

### 总结

通过实际案例了解数据科学研究流程,同时注重项目的伦理影响。开展研究要结合多个专业,实现问题的全面判定。数据科学能为各个领域提供支持,但同时需要谨记其影响及限制。

## 26. Lecture 27 (Final Review 1) - Data 100 Su19

### 1. 概述

本讲通过问题和代码练习形式回顾了SQL、React、可视化、PCA、概率分布、损失函数、逻辑回归分类器、决策树和随机森林等课程内容。

### 2. SQL和React

通过true-false问题和简单的代码练习回顾了SQL和React。

### 3. 可视化

主成分分析(PCA)用于降维和数据可视化。PCA查找数据最大方差的方向作为第一主成分,第二主成分与第一主成分正交。

### 4. PCA

PCA是无监督方法。它搜索数据最大方差的方向。PCA后的特征可能无法解释,但至少保留原始数据的最大方差。

### 5. 概率分布

概率变量采样类型例子:若采样只选定一个字母序列的房屋,则为簇抽样。根据不同字母序列房屋偏好情况,整体偏好为3/4。估计值平均值为3/4。

### 6. 损失函数

损失函数定义了n个样本每个维度D的数据。它适用于分类问题线性模型。增大λ值将增加偏差但降低方差,实现过拟合控制。

### 7. 逻辑回归分类器

通过问题回顾逻辑回归分类器。

### 8. 决策树与随机森林

通过更详细的讨论回顾了决策树和随机森林。

## 27. Lecture 28 (Final Review 2) - Data 100 Su19

### 模型概述

建立模型的目的是利用已有数据集来预测其他未知的数据。不同的模型可以达到同样的预测效果,但是为了评价模型的好坏,需要采用损失函数。损失函数可以衡量模型对整个数据集的预测效果。损失值越低代表模型效果越好。

### 多变量线性回归

简单线性回归只能用一个变量进行预测。当数据集包含多个预测变量时,需要采用多变量线性回归模型。多变量线性回归将每个预测变量看作一个纬度,通过求每个预测变量的斜率来找到对整个数据集最佳的预测效果。

### 特征矩阵

特征矩阵可以把数据集中的每一行观测作为一个样本,每一列作为一个特征变量。模型通过学习整个特征矩阵,来预测结果变量的值。

### 线性回归优化

采用平方损失函数可以计算整个数据集的平均损失。为了找到最佳的模型参数theta,需要通过对损失函数求导,并使其等于0来求解。这可以得到θ的最优解θ^*。但是特征矩阵X需要满足可逆性条件,才能求得θ^*。

### 特征工程

特征工程 是通过对原有特征变量进行操作,来生成新的特征变量,例如将数量特征平方、取对数,或者将多个特征相乘等。这可以让模型学习更复杂的关系。

对于分类特征变量,需要将其转为数值形式代表。同时需要注意分类特征之间没有大小关系。

### 问题与回答

在解释过程中没有出现任何问题或需要回答的内容。
