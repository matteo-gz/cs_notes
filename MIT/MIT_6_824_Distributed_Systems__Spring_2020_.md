# MIT 6.824 Distributed Systems (Spring 2020)
>
> <https://www.youtube.com/playlist?list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB>

## 1. Lecture 1: Introduction

### 1.1 计算机视觉概述

计算机视觉是计算机科学的一个分支,它利用电脑对数字图像进行分析和理解从而实现智能视觉。计算机视觉操作的对象主要是数字图像,它可以实现图像的检测、识别和三维重建等功能。

### 1.2 计算机视觉领域

计算机视觉的主要应用领域包括:

1. 图像处理。对数字图像进行滤波、增强和重构等操作。

2. 图像检测。例如运动对象检测和人脸检测等。

3. 图像识别。例如光学字符识别(OCR)和目标识别等。

4. 三维重建。通过图像信息重建三维场景的立体几何结构。

5. 机器视觉。将计算机视觉技术应用于自动驾驶、机器人触觉等领域。

### 1.3 计算机视觉任务

计算机视觉的主要任务包括:

1. 图像分割。将图像分成不同的区域或物体。

2. 物体检测。识别图像中的物体并标记其位置。

3. 物体识别。判定物体的类别标签。

4. 3D重建。通过单个或多幅图像估计场景的几何结构。

5. 活体检测。判断图像中的物体是否是真实的生物。

6. 光流估计。跟踪图像序列中的运动目标。

7. 识别与跟踪。长期且连续地跟踪物体的运动与变化。

以上就是计算机视觉的基本概述,下一讲将详细介绍计算机视觉任务的实现方法。

## 2. Lecture 2: RPC and Threads

### 2.1 RPC概念

RPC(Remote Procedure Call)即远程过程调用,是一种进程间通讯方式。它使一个程序可以像调用本地序列一样简单地调用另一个地址空间(通常在不同主机上的)的程序,即远程子程序。

### 2.2 RPC工作原理

RPC工作的基本流程:

1. 客户端调用本地的stub函数,将参数打包传递给服务器。

2. stub函数负责将调用翻译成能够进行网络传输的消息格式。

3. 网络传输层将打包后的消息发送到服务器。

4. 服务器端的stub函数收到消息后进行解包,还原成原来的调用 request。

5. 服务器端调用具体实现的remote procedure,取得结果返回。

6. 服务器端stub函数再次进行封装,通过网络返回结果给客户端stub。

7. 客户端stub函数将结果translated成原来调用的返回值。

### 2.3 RPC套接字通信

两种常用的RPC通信机制:

1. TCP/IP连接式 - 建立可靠的双向连接通道进行通信。

2. UDP连接less - 无连接通信,数据包可能丢失或顺序错乱,需要自己实现流程控制和重传。

### 2.4 线程概念

线程是操作系统能夠进行运算调度的最小单位,它们使得同一个程序可以同时执行多个任务。

与进程不同,同一个程序的多个线程共享程序的文本段、数据段和堆栈段。线程间上下文切换开销小。

多线程程序可以利用多核CPU的威力,对计算资源利用率高。常见多线程模型有多线程、服务器线程池模型等。

以上是RPC和线程相关的基本概念介绍。

## 3. Lecture 3: GFS

### 3.1 GFS概述

GFS(Google文件系统)是Google开发的分布式文件系统。GFS被设计用于运行在大规模集群上,提供高达PetaBytes级的存储容量和高吞吐量访问能力。

### 3.2 GFS设计

GFS采用主从master-slave的架构设计:

1. Master负责管理整个文件系统的交易日志和文件名空间。

2. 文件被分割为块存储在Slave上,每个Slave管理存储在其本地磁盘的文件块。

3. 文件被分割为较大尺寸的块(64MB默认),每个块是一个独立的文件存储对象。

4. 元数据(文件属性、权限等)被保存在Master上,块数据保存在多个Slave上。

5. 文件系统提供了Append写的能力,实现高并发写入。重复和失败的写入自动被GFS优化。

### 3.3 GFS Read/Write

GFS的读写工作流程:

1. 客户端向Master请求文件块的位置。

2. Master返回块保存位置(Slave地址)。

3. 客户端直接与存储Slave通讯下载/上传文件块。

4. 所有操作都被记录在Master的事务日志中,从而提供软件冗余和恢复功能。

GFS采用此设计能提供高可靠性、高扩展性和高可用性的分布式文件服务。

## 4. Lecture 4: Primary-Backup Replication

### 4.1 主备复制概述

主备复制是数据库管理系统中一种常见的数据复制策略,目的是实现数据的高可用性。

### 4.2 主备复制工作模式

主备复制中包含:

1. 主节点(Primary):处理所有客户请求,并向备节点同步 committing 事务。

2. 备节点(Backup):定期从主节点pull同步committed事务写入自己本地,处于passive/readonly状态。

3. 当主节点故障时,任一备节点通过选举升级为新的主节点。

### 4.3 主备复制优缺点

优点:实现了高可用性;当主节点出故障时可以快速切换至备节点提供服务。

缺点:写入性能受限于同步复制带来的延迟;单点失效问题,如果主备节点同时失效会导致部分数据丢失。

### 4.4 主备复制应用举例

此策略常见应用:

1. MySQL主从架构

2. MongoDB副本集

3. Redis集群

4. ZooKeeper quorum节点

主备复制简单实用,适用于读 relatively 更频繁的场景,但单点故障容错能力相对较弱。

## 5. Lecture 5: Go, Threads, and Raft

### 5.1 Go语言线程模型

Go支持轻量级线程(goroutine),每个goroutine都占用很少的内存,切换速度快。

Go采用M:N模型,一个操作系统线程可以 schedules多个goroutine,充分利用CPU。

并发利用channel做同步和通信。

### 5.2 Go类型的协程

Go语言中的协程类型主要有:

- Goroutine:轻量级协程,运行在OS线程上。

- OS Thread:操作系统中的重量级线程。

- Goroutines多于OS线程,通过M:N模型更高效利用资源。

### 5.3 Raft一致性算法

Raft是分布式一致性协议的一种,能在不同服务器之间保持数据的一致性。

Raft的主要思想:

1. 选举领导者 leader 控制一切

2. 提交日志replicated到多数节点

3. 故障检测和项目恢复

4. 动态领导人转移

Raft广泛应用于分布式系统一致性保证。

以上为Go语言线程模型和Raft一致性算法的基本概述。

## 6. Lecture 6: Fault Tolerance: Raft (1)

### 6.1 Raft一致性算法概览

Raft通过进行日志复制和leader选举来保证系统的一致性。

### 6.2 Raft状态机模式

Raft系统每个server都维持一个状态机,状态包括:

1. 当前term

2. 待选leader id

3. log日志

其中log日志是顺序append-only的重要数据结构。

### 6.3 Raft工作模式

1. leader发送心跳保持主导权

2. client向任何server发送请求

3. server判断是否有leader,如无选举产生新leader

4. leader将日志附加到本地日志,复制给followers

5. 日志同步后leader提交请求,返回结果给client

### 6.4 Raft一致性要求

1. 只有一位leader

2. leader失效需快速选举产生新leader

3. 日志同步要求日志冗余存储在多数机器上

以上介绍了Raft的基本模型、工作模式和一致性要求,下一讲将详细介绍Raft的选举和日志同步机制。

## 7. Lecture 7: Fault Tolerance: Raft (2)

### 7.1 Raft选举机制

1. 心跳机制,leader定期广播选举消息Term

2. 接收广播的server比对Term

3. 如Term比自己小或接受广播时间超过选举时间窗,触发选举

4. 选举产生新leader,同时更新Term

### 7.2 Raft日志复制

1. 客户端请求写日志到当前leader

2. Leader将日志附加到本地,并复制给followers

3. 确认日志同步到多数节点后,leader将日志提交,返回给客户端  

4. 日志同步分为预投票和正式投票阶段确保一致性

### 7.3 Raft负载均衡

Raft在保证一致性的基础上,提供以下负载均衡机制:

1. 客户端可以随机连接不同server点对点分流请求  

2. Leader重分配日志 Region给不同Followers倾斜负载

3. 动态Leader转移平衡全局负载

Raft通过上述机制在高可用性的同时也提供一定程度的负载均衡能力。

## 8. Lecture 8: Zookeeper

### 8.1 Zookeeper简介

Zookeeper是专门为分布式应用程序提供协调服务的开源项目,能提供配置维护、名称服务、分布式同步等功能。

### 8.2 Zookeeper数据模型

Zookeeper采用类文件系统的阶层模型,最大程度上支持并行化和分布式。

服务器上的数据结构为 znode 节点,对应于文件系统中的文件与目录。每个 znode 都有唯一路径。

### 8.3 Zookeeper数据管理

Zookeeper支持访问控制及监视机制,客户端可以实现:

1. 创建/删除/修改 znode

2. 获取子节点列表

3. 判断存在/数据是否更新

4. 设置节点数据的监视器

### 8.4 Zookeeper集群与选举

Zookeeper采用主备模式构建集群,通过选举产生单位 Leader 节点进行统一协调。

提供高可用性和线性一致性,应用可以基于它实现分布式锁、队列等服务。

以上为Zookeeper的基本功能及工作原理概要,具体还包括会话管理等内容。

## 9. Lecture 9: More Replication, CRAQ

### 9.1 主从复制与多主写模式差异

主从复制只读性能好,单点损害;多主写可以写,但一致性难保证。

### 9.2 CRAQ一致性算法

CRAQ是一种多主写算法:

1. 建立一致性视图,每个副本维护一致性时间戳

2. 读请求 通过任意副本返回结果

3. 写请求带时间戳打到所有副本,副本评估时间戳顺序执行

4. 通过仲裁方达成一致后返回确认

### 9.3 CRAQ优点

1. 写可用,不单点损失

2. 一致性时间戳机制,保证线性一致性

3. 读取任意副本,性能高

4. 算法简单,实现难度低

CRAQ 松弛了顺序一致性,折中主从与悲观锁模型,提供线性一致性。

### 9.4 CRAQ应用场景

适用于需要高可用性和高并发读写的场景,比如配置管理、消息队列等。

## 10. Lecture 10: Cloud Replicated DB, Aurora

### 10.1 云原生分布式数据库

面向云环境构建的数据库需要:

1. 高可扩展性

2. 自动故障转移

3. 自动备份恢复

4. 多区域数据一致性

5. 监控告警

6. 弹性扩容缩容

### 10.2 Aurora数据一致性协议

1. 建立多机房副本集群

2. 引入全局排序的单调时钟

3. 写请求按时钟顺序同步各区域

4. 读任意可用副本

5. 通过活锁检测机制防止环路依赖

### 10.3 Aurora其他特性

1. 自主存储,跨主机存储副本

2. 低损失故障转移,30s内完成切换

3. 自动扩容缩容,按需水平弹性伸缩

4. 监控告警,harmonic分析性能瓶颈

Aurora实现了MongoDB兼容的多区域数据一致性解决方案。

## 11. Lecture 11: Cache Consistency: Frangipani

### 11.1 缓存一致性问题

分布式缓存必须保证读写一致性。常见算法有:

1. 强一致性:实时重新加载数据,性能影响大

2. 松弛一致性:允许陈旧数据读取,需要应用负责监控

### 11.2 Frangipani缓存协议

Frangipani采用时间戳机制提供程序可配置的一致性:

1. 数据对象有全局时间戳版本

2. 缓存和后端存储维护时间戳

3. 读请求根据时间戳返回最新或陈旧数据

4. 写请求更新所有节点时间戳

### 11.3 Frangipani优势

1. 提供一致性可配置性

2. 不需要同步刷新机制,性能好

3. 可检测脏读问题

4. 实现了一致性松弛

5. 适用于缓存无序感知应用

Frangipani是一个灵活高效的分布式缓存一致性解决方案。

## 12. Lecture 12: Distributed Transactions

### 12.1 分布式事务问题

分布式系统跨数据库、服务进行事务处理时面临以下问题:

1. 本地事务提交,子事务失败导致其它资源已被占用(不一致)

2. 网络断连导致长时间阻塞或事务中途无法确定结果

3. 事务回滚性能不好,需要回滚所有参与分布式事务的资源

### 12.2 2PC协议

2相肯定协议:

1)询问阶段:事务协调者询问所有资源是否准备好Commit

2)提交阶段:如都准备好,事务协调者指令所有的资源Commit

3)如有节点未准备,全部回滚

### 12.3 3PC协议

三相肯定协议:

1)预提交阶段:协调者询问准备,返回是否准备好预提交

2)预提交阶段:如未发生异常,发送预提交指令

3)提交阶段:如预提交成功,发送最终提交指令

3PC更复杂但优于2PC解决了暂语句问题

以上为分布式事务常见问题与2PC/3PC协议的介绍。

## 13. Lecture 13: Spanner

### 13.1 Spanner 并发控制

Spanner使用并发锁解决分布式事务问题:

1. 数据行添加时间戳控制版本

2. 事务操作行时上读锁和写锁

3. 锁定粒度为行

4. 避免死锁通过等待时间戳超前行获取锁

### 13.2 Spanner 强一致读

Read只从当前最新版本读,实现强一致性:

1. 多数据中心副本同步

2. 全局有序时间戳 TrueTime

3. 根据时间戳选择最新副本读取

### 13.3 Spanner分布式事务

采用两阶段提交2PC协议,实现强一致性全局分布式事务:

1)预提交阶段

2)数据确认提交阶段

通过行锁和时间戳版本控制,解决分布式事务一致性问题。

Spanner提供强一致读写能力,是公认的分布式事务管理系统典范。

## 14. Lecture 14: Optimistic Concurrency Control

### 14.1 乐观锁与悲观锁区别

悲观锁:获取锁后才执行事务。

乐观锁:假设不会产生冲突,在提交时检查是否有冲突。

### 14.2 乐观锁工作机制

1. 每次事务读取数据时会同时读取版本号

2. 事务在本地计算修改后的数据和版本号

3. 提交时比较当前版本号与事务开始时的版本号

4. 如版本号一致,则更新数据和版本号

5. 若版本号不同,事务有冲突,通知回滚重试

### 14.3 乐观锁优点

1. 极少阻塞,并发度高

2. 冲突在提交时才检测,开销小

3. 性能比悲观锁好,尤其读多写少的场景

4. 可以使用多版本控制扩充锁定粒度

乐观锁通过非阻塞方式提高并发性,在应用场景不同时,优先考虑使用。

## 15. Lecture 15: Big Data: Spark

### 15.1 Spark介绍

Spark是一个快速、通用的大数据处理引擎。支持的计算模型包括:

- RDD:弹性分布式数据集,内存计算,容错性优秀。

- DataFrame:基于RDD构建的表结构型Dataset接口。

- Dataset:强类型数据抽象层,提供SQL解析能力。

### 15.2 Spark优势

- 内存计算速度快,支持高性能迭代算法。

- 与Hadoop集成好,对各种数据源有很好支持能力。

- 丰富的第三方库支持多种计算模式,如图计算、机器学习等。

- 提供交互式查询能力,快速开发分析应用。

### 15.3 Spark架构

常见部件包括Driver程序、集群Manager(Spark Context)、Executor节点和存储系统。

通过Task分片的方式支持双重如地域分布式处理。

Spark提高了内存计算能力,适用于互联网、金融等数据分析应用。

## 16. Lecture 16: Cache Consistency: Memcached at Facebook

### 16.1 Facebook使用Memcached的场景

Memcached在Facebook主要用于:

- 用户信息缓存
- 新闻动态缓存
- 好友关系缓存
- 访问统计缓存

处理热点数据查询和缓减后端数据库压力。

### 16.2 Facebook Memcached一致性方案

- 数据对象设置有效期,主动过期采用软删除方式。

- 缓存与数据库双写模式,保证数据最终一致。

- 实现Memcached和MySQL的打通通知机制,如MySQL过期自动通知Memcached删除。

- 客户端重试策略,一致性读取尝试从Memcached与MySQL查询。

### 16.3 主要优化

- 水平可伸缩的Memcached集群和客户端客户端连接池。

- 对象结构简单,按需加载字段实现更细粒度的缓存。

- 集群容量规划、监控与报警机制。

Facebook通过简单有效的方案,很好地在Memcached基础上实现了缓存一致性。

## 17. Lecture 17: COPS, Causal Consistency

### 17.1 COPS系统

COPS是一个推送式分布式系统:

- 节点之间通过订阅关系形成逻辑递推关系图

- 数据更改会被推送给下游订阅节点

- 下游节点将更新应用到内部状态

### 17.2 COPS一致性模型

COPS采用因果一致性模型:

- 每个数据元素有版本向量代表其因果联系

- 读取按版本向量中的版本读取

- 写入会把自己的版本向量推送给依赖节点

### 17.3 COPS特性

- 具备推送通知机制,适合实时数据同步

- 因果关系表达清楚,一致性要求低

- 支持分区读写,横向扩展简单

- 适用于社交、消息、报表等应用场景

COPS通过顺应业务流向的方案实现了一致性松弛。

## 18. Lecture 18: Fork Consistency, Certificate Transparency

### 18.1 叉一致性模型

叉一致性允许因版本分叉导致的不一致状态存在:

- 数据修改形成新分支版本
- 读可以选择任意版本结果
- 总能读到最近的修改

### 18.2 Certificate Transparency

数字证书提供给客户端站点验证,防止中间人攻击。

CT实现数字证书透明:

- CT日志记录网站证书
- 任何人可查询日志验证证书
- 浏览器检查日志确认证书颁发
- 找不到日志则证书无效

### 18.3 CT工作原理

- 日志服务接受证书提交并签名证明
- 客户端定期从日志下载校验证书
- 日志间同步监测证书变动
- 提供网站回放功能进行审计

CT通过公开记录机制提高SSL证书的可信度和透明度。

## 19. Lecture 19: Bitcoin

### 19.1 比特币设计原理

1. 使用区块链记录所有的交易记录

2. 每个用户通过数字签名进行数字货币的所有权验证

3. 挖矿激励机制使网络参与者能够共识维护系统

4. 使用密码哈希来保证网络上每一个区块的chronicle顺序

### 19.2 比特币技术架构

1. P2P点对点网络架构

2. 使用Merkle树结构组织交易信息

3. 加密货币采用数字签名技术防止重放攻击

4. 挖矿难度自动调节保证生成新区块间隔大约为10分钟

5. 使用浪费性工作量证明算法选择出下一个生成新区块的节点

比特币以去中心化的方式实现了金融系统,对区块链技术的发展也有重要影响。

## 20. Lecture 20: Blockstack

### 20.1 Blockstack概述

Blockstack是一套去中心化的身份管理系统,基于比特币区块链:

- 用户采用公私密钥对生成唯一的用户ID

- 用户数据以名称-值对的形式存储在区块链上

- 编码用户数据用于提供可扩展存储空间

### 20.2 域名解析

- 域名解析信息通过区块链实现不可篡改

- 域名包含用户公钥和加密数据地址

- 数据存储在IPFS协议的分布式文件系统上

### 20.3 应用开发

- Blockstack提供多语言SDK实现身份认证

- 通过用户名密码或数字签名登录认证

- DAPP可直接访问区块链AUTHENTICATE记录

Blockstack通过去中心化机制,实现了网络身份的自主和可控。
