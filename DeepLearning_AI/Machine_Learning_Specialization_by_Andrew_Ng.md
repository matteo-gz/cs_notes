# Machine Learning Specialization by Andrew Ng
>
> <https://www.youtube.com/playlist?list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI>
# 1 Machine Learning Specialization [Course 1, Week 1, Lesson 1]

### 什么是机器学习

机器学习是一门科学,它可让计算机在没有明确编程的情况下自行学习。机器学习的应用十分广泛,您每日都可能使用其中的一些项目而不自知。

例如通过谷歌搜索问题或必应展示附近餐厅就是机器学习。社交软件通过人脸识别自动标记朋友,音乐流媒体服务则通过历史记录推荐类似音乐。

语音助手像Siri能够自动识别语音指令。电子邮件过滤函数能够过滤广告邮件。

除消费类应用外,机器学习也广泛应用于工业和医疗领域。例如它可以帮助风力涡轮机优化发电,医生提升诊断能力。在工厂,它通过视觉检查发现产品缺陷。

### 这个课程概述

本课程将教授机器学习的基本概念并让学生实际应用。学生将学习如何使用代码实现机器学习算法,解决现实问题。

过去许多人通过这门课程实现了机器学习项目,甚至开始了AI从业生涯。希望通过这个系列的学习,学生们也能掌握机器学习的基础知识并在AI领域有所作为。
# 2 Machine Learning Specialization [Course 1, Week 1, Lesson 1]

### 机器学习发展历史

机器学习起源于人工智能领域,目的是建立智能机器。早期可以教会机器实现简单任务,但难以为更复杂任务编写程序,如网络搜索、语音识别等。

自20世纪以来,通过让机器自学来解决问题的思路日益流行。例如语音识别和图像识别等应用都取得良好效果。

### 机器学习应用广泛

如今,机器学习技术已遍布各行各业。许多人通过研究各类应用掌握了机器学习技能,并在AI领域有所作为。

包括Google、Facebook等公司,都广泛使用机器学习解决问题。个人也可通过学习这门课程掌握基本概念和算法,实现不同项目。

### 机器学习未来发展

机器学习还有很多未开发应用领域,预计将带来巨大经济效益。随着技术进步,一些人试图实现人工总体智能,但这仍需要很长时间的研发。

目前,机器学习领域需求极为巨大。正是学习这些技能的好时机。本课程将探讨机器学习的定义、主要算法分类与应用场景。
# 3 Machine Learning Specialization [Course 1, Week 1, Lesson 2]

### 超分类与非监督学习定义

超分类学习(supervised learning)是机器学习中应用最广泛的分类,它具有输入和结果数据。算法通过分析输入输出关系来学习。

非监督学习(unsupervised learning)没有结果标签,仅根据输入数据来发现数据中的隐含模式和结构。

### 超分类学习主要应用场景

超分类学习广泛应用在许多领域,如图像识别、语音识别、医疗诊断等。它包括回归分析与分类两种方法。

回归分析预测连续值结果,如房屋价格。分类预测离散类别结果,如邮件是否垃圾邮件。

### 非监督学习主要应用场景

非监督学习用于数据挖掘,比如市场分析、天文学图像处理等。它找出输入数据的内在结构,如聚类分析将数据分组。

这门课程分三个阶段专注不同机器学习方法,第一二阶段着重超分类学习,第三阶段关注非监督学习与其他方法。掌握这些概念有助于选择合适算法解决问题。
# 4 Machine Learning Specialization [Course 1, Week 1, Lesson 2]

### 监督学习概念

监督学习(supervised learning)是机器学习中应用最广泛的分类方式。它需要提供输入X和期望输出Y的训练样本对,让算法学习输入输出之间的映射关系。

### 监督学习主要应用

监督学习广泛应用于分类和回归问题。分类预测离散结果,如垃圾邮件过滤。回归预测连续值结果,如房价预测。

许多成功案例都使用了监督学习,例如语音识别、机器翻译、在线广告等。都需要提供带标签的训练样本。

### 回归问题实例

以房价预测为例,给出房屋面积和价格作为输入输出对。通过拟合直线或曲线来预测新房屋价格。这是一个回归任务,目标是预测一个连续值。

监督学习的优点是可以通过学习真实输入输出关系。但需要大量标记数据支持训练。它解决了人工难以给出明确算法的问题。

### 分类问题概念

除回归外,监督学习的另一主流任务是分类。它预测离散类别结果,如垃圾邮件分类为垃圾或正常邮件。不同监督学习问题选择不同算法。
# 5 Machine Learning Specialization [Course 1, Week 1, Lesson 2]

### 监督学习算法学习输入输出关系

监督学习算法通过学习训练数据集中的输入(X)与期望输出(Y)之间的映射关系,来预测新的输入的输出。

### 回归与分类案例对比

回归算法会预测任意连续值,如房价预测。分类算法只输出有限个类别,如肿瘤是否恶性。

### 肿瘤恶性判断作为分类问题示例

通过学习医疗诊断记录,预测肿瘤是否恶性。标签有0(良性)和1(恶性)两类。此外还可以分辨不同类型恶性肿瘤。

### 分类算法特征

分类算法预测有限个类别,类别可以是数字或非数字。可以根据一个或多个特征来分类,如根据年龄和肿瘤大小判断。算法 learned 边界来分类新样本。

### 监督学习概述

监督学习通过学习训练数据实例标签,来建立输入与输出之间的映射。它主要包括回归和分类两种类型问题。分类预测有限类别,回归预测任意连续值。
# 6 Machine Learning Specialization [Course 1, Week 1, Lesson 2]

### 无监督学习定义

无监督学习(unsupervised learning)是机器学习的一种主要形式,它没有结果标签,仅根据输入数据来发现数据中的隐含模式和结构。

### 聚类算法

聚类算法(clustering algorithm)是无监督学习的一种,它可以将未标记的数据自动分组到不同的簇中。

### 新闻文章聚类示例

Google新闻使用聚类算法将每日大量新闻文章聚合到主题相同的簇中,如将熊猫相关文章聚到一起。

### 基因微阵列数据聚类

聚类算法也可以将DNA数据中的个体根据基因表达模式分为不同类型。

### 客户市场细分

公司可以通过聚类客户数据自动发现不同客户群体,以更好地服务客户。

### 深度学习社群细分

研究团队通过聚类深度学习社区成员的数据,发现其学习动机有知识增长、职业发展和跟踪行业趋势等类型。

### 其他无监督学习算法

除聚类外,还有其他无监督学习算法用于数据规约、降维等,将在后续内容中进行介绍。
# 7 Machine Learning Specialization [Course 1, Week 1, Lesson 2]

### 无监督学习定义

无监督学习(unsupervised learning)的数据仅包含输入X,而没有输出标签Y。算法需要在数据中找到结构、模式或有趣的内容。

### 无监督学习算法类型

除聚类(clustering)外,本专项还学习到两个类型的无监督学习算法:

1. 异常检测(anomaly detection)用来检测不寻常事件,在发现欺诈交易中很重要。

2. 维度减少(dimensionality reduction)可以将大数据集压缩到更小的数据集,同时保留尽可能多的信息。

### 无监督学习问题范例

新闻文章聚类、客户市场细分可以作为无监督学习问题。而垃圾邮件过滤和糖尿病诊断需要输出标签,是监督学习问题。

### 后续授课内容

专项会更深入探讨异常检测和维度减少算法。此外,Jupyter Notebook在机器学习中的重要性也将在下课介绍。
# 8 Machine Learning Specialization [Course 1, Week 1, Lesson 2]

### Jupyter Notebook

Jupyter Notebook是机器学习和数据科学工程师广泛使用的开发环境。它可以在浏览器中运行。

### Notebook内容

Notebook包含文本描述细胞(Markdown)和代码细胞(Code)两种格式。选择Markdown细胞后可以修改文本,选择Code细胞后可以运行Python代码。

### 选做实验

选做实验中提供了只读的代码实验,学生可以直接运行代码观察结果,不需自己编写代码。这有助于理解机器学习算法和代码运行机制。

### 课后实验

从下周开始会提供能编写代码的课后实验。学生可以在Notebook中修改运行代码,深入研究机器学习知识。

### Notebook使用方法

学生可以在浏览器内打开Notebook,选择细胞运行代码,预测并观察结果。也可以修改代码探索结果变化。这将有助于熟悉Python编程环境。
# 9 Machine Learning Specialization [Course 1, Week 1, Lesson 3]

### 训练数据标准表示法

训练数据表示为(x,y)对,其中x代表输入特征,y代表输出目标值。

### 训练数据集表示

训练数据集包含m个训练样本。用小写m表示样本数,用(x^i,y^i)表示第i个样本,其中i∈[1,m]。

### 房价预测实例

例如房价预测中,x代表房屋面积,y代表房屋价格。训练数据集包含47个样本,每个样本记录一个房屋的面积和价格。

### 算法学习过程

算法会从训练数据集中学习输入输出关系,建立预测模型。然后可以用得到的模型预测新数据的输出,如预测未售房屋的价格。

### 线性回归模型

视频演示使用线性回归算法学习房屋数据,拟合出一条直线来描述面积和价格的关系,用来预测新房屋的价格。

### 监督学习概念

监督学习需要提供带有标签的训练数据。算法从大量输入输出样本对中学习规律,建立通用模型,用于新输入的预测。
# 10 Machine Learning Specialization [Course 1, Week 1, Lesson 3]

### 监督学习过程

监督学习算法需要训练数据集(输入x,输出y)进行训练,输出一组函数f。f能够输入新的x,输出预测结果ŷ。

### 线性回归模型

视频使用线性回归算法,将函数f表示为fx=w×x+b的形式,其中w和b为学习的参数。模型根据训练数据集学习w和b的值,求得预测函数f。

### 训练数据集

训练数据集包含m个样本(xi,yi),i∈[1,m]。xi表示第i个样本的输入特征,yi表示目标输出值。

### 预测过程

给定新样本的输入x,模型利用学习得到的函数f计算预测输出值ŷ。ŷ尝试预测样本的真实输出y,二者可能不是完全一致。

### 代价函数

要使模型学习的参数w和b最优,需要构建代价函数。代价函数衡量预测值ŷ与真实值y的差距,模型学习是来最小化这个差距。

### 线性回归示例

视频使用房价预测作为例子,以房屋面积x作为输入特征,价格y作为输出目标。采用直线模型fx=wx+b进行预测。
# 11 Machine Learning Specialization [Course 1, Week 1, Lesson 3]

### 定义代价函数

为实现线性回归,第一个关键步骤是定义一个称为代价函数的东西。代价函数可以衡量模型的表现效果,以便优化模型。

### 训练数据表示

训练数据集包含输入特征X和输出目标Y,每一个训练样本用(xi,yi)表示,其中i∈[1,m],m是样本数。

### 预测过程

函数f根据参数W和B拟合训练数据集,对新输入x进行预测,得到估计输出ŷ。ŷ尝试与真实输出y匹配。

### 参数W和B

W和B称为模型的参数。它们决定函数f的形式,从而影响预测结果。不同W和B对应不同直线,拟合程度也不同。

### 代价函数表达式

代价函数J(W,B)计算预测误差(ŷ-y)的平方和,即最小均方误差。经过计算后得到J(W,B)=1/(2m)Σ(f(xi)-yi)2形式。

### 代价函数意义

代价函数表明参数W和B使预测与真实值匹配程度。J值越小,预测效果越好。为寻找最优模型参数,需要最小化J函数。

### 下一步

下一视频将通过例子介绍代价函数的具体计算过程,进一步提高对其含义和最小化算法的理解。这有助于解读线性回归的数学原理。
# 12 机器学习专项课程[课程1,第1周,第3课]

### 成本函数

成本函数(Cost Function)用来衡量模型参数和训练集之间的差异程度。选择使成本函数最小化的参数值,可以得到一个拟合数据较好的模型。

简化线性回归模型使用单个参数W,模型为f(x) = Wx。成本函数定义为:

J(W) = (1/2m)Σ(f(xi) - yi)2

其中m是训练集大小,xi是特征,yi是标签。

利用这个简化模型,可通过 varied W值,画出f(x)和J(W)两个函数的关系图。

- 当W=1时,f(x)拟合数据最好,J(W)最小,值为0
- 当W从1增大或减小时,f(x) fitting效果越差,J(W)值越大
- 所以模型的参数W应取使J(W)值最小的位置,即W=1

通常线性回归模型使用两个参数W和b,模型为f(x) = Wx + b。同样,目标是找使J(W,b)最小的参数值(W,b)。

总之,成本函数可以量化模型误差大小,线性回归通过求解使成本函数最小化的参数,来寻找最优拟合数据的模型。
# 13 Machine Learning Specialization [Course 1, Week 1, Lesson 3]

### 代价函数的3D曲面图

上一视频见到代价函数J用一维函数来表示,此处用三维曲面图表示J作为W和B两个参数的函数。

### 等高线图

等高线图通过水平切片三维曲面获取相同高度的点,以二维图表示曲面每点的J值。

### 等高线图示例

提供地形图作为等高线图例子。3D曲面下方为房屋预测例子中的J曲面,右上为它的等高线图。

### 等高线图解析

等高线为椭圆形,表示位置不同但J值相同的点。J最小处对应的是等高线最小椭圆中心点。

### 代价函数形状

不管是3D曲面还是等高线图,J的形状类似“碗”或“馒头”。求解线性回归模型时需要找到使J最小的W和B点。

### 下一步内容

随后视频将用不同的W和B值展示预测函数f(x)的变化情况,进一步理解模型训练的本质是找到最小化J的W和B参数。
# 14 Machine Learning Specialization [Course 1, Week 1, Lesson 3]

### 线性回归模型几个选择

视频通过示例展示了线性回归模型不同参数w和b选择得到的函数f(x)及其代价J值。

### 观察f(x)与数据点匹配程度

可以观察不同f(x)线与训练数据点之间的拟合程度。 closest match 程度对应J值最小。

### 代价函数图形表示

J值可以用等高线图或3D曲面表示,标记出不同w和b对应的J点。J最小处对应最佳拟合模型。  

### 交互式绘图示例

可选实验包含交互式曲线图,点击获取对应w和b下f(x)线及J点,观察关系。

### 算法训练核心

手动搜寻图形无法扩展复杂模型。需要高效算法如梯度下降自动找最小J,获取最佳w和b参数。

### 下一步内容

将详细介绍梯度下降算法,它常用于线性回归与神经网络等各种模型参数优化,是机器学习重要算法之一。
# 15 Machine Learning Specialization [Course 1, Week 1, Lesson 4]

### 梯度下降算法

梯度下降算法可以用于最小化任何函数,包括线性回归的代价函数以及其他模型更复杂函数。

### 初始猜测

算法从一组随机或预设的W和B参数值开始,作为函数J的初值点。

### 每步更新

重复多次:计算当前点的梯度方向,以小步长沿此梯度方向更新W和B,使函数值下降最大。

### 梯度方向

用函数对W和B的导数表示梯度方向,即导数最大增长的方向,对应着下降最陡的方向。

### 迭代收敛

重复更新后,函数值将趋于一个本地最小值点。但可能存在多个局部极小值,取决于初始点。

### 示例解释

通过山岭上下降的比喻,解释梯度下降在函数空间的数学原理和每个迭代步的物理意义。

### 本地极小值

同一函数可能存在多个极小值点。算法结果依赖起始点,可能导致不同的局部最优解。

### 下一步内容

详细介绍梯度下降的数学推导,以便理解其实现的核心思想。
# 16 Machine Learning Specialization [Course 1, Week 1, Lesson 4]

### 梯度下降算法

梯度下降算法是用于最小化代价函数J,寻找模型的参数W和B。

### 更新参数公式

W和B按下式更新:

W = W - α×∂J/∂W
B = B - α×∂J/∂B

α是学习率,控制步长大小。∂J/∂W和∂J/∂B是代价函数对W和B的导数,指出下降的方向。

### 同时更新重要

必须同时更新W和B。正确方式是:

1. 求取W和B新的临时值
2. 将临时值同时赋值给W和B

反之为非同时更新,算法效果会不同。

### 梯度下降流程

重复更新W和B,直到J收敛于极小值点,参数W和B变化不大为止。

### 导数概念

导数来自函数概念,即函数在某点斜率,指出变化趋势。但不需要深入掌握,下一视频将详细解释。
# 17 Machine Learning Specialization [Course 1, Week 1, Lesson 4]

### 一维梯度下降示例

使用一个参数W的代价函数J(W)来更好解释梯度下降算法。

### 梯度下降公式

W更新为W - α×dJ/dW,其中α是学习率,dJ/dW是J在W处的导数,指出下降方向。

### 导数含义

导数在某点表示切线斜率。通过绘制切线确认导数的正负性。

### 更新方向

如果导数正,W下降;如果导数负,W上升,都使J值下降。

### 不同初始化点效果

从不同W0开始,梯度下降每次都使J值下降,收敛到极小值点的右侧或左侧。

### 导数角色

导数决定W的更新方向(上升或下降),使算法遵循函数下降最快的方向进行优化。

### 学习率α影响

学习率控制更新幅度。后续将详细讨论学习率选择对结果的影响。

### 总结

通过图形例子理解梯度下降中导数和学习率在参数优化中的作用机制。
# 18 Machine Learning Specialization [Course 1, Week 1, Lesson 4]

### 学习率大小影响

学习率α决定梯度下降更新参数的幅度,α大小会直接影响算法收敛性能。

### α太小情况

α太小导致每次更新步伐太小,需要大量迭代才能接近极小值,效率低下。

### α太大情况

α太大可能一次更新过大跳过最优解,甚至越离极小值越远,算法无法收敛。

### 最小化示例分析

通过绘图分别展示α太小和太大的情况,解释更新过程和结果。

### 极小值点更新动作

当已达极小值时,导数为0,更新不变保持原点。这解释了固定α下能达极值原因。

### 隐含学习率

随着迭代,导数趋于0,步伐自动缩小,等效于α在降低,这是自动调整步伐的重要机制。

### 线性回归算法

下一步将具体使用均方误差作为代价函数J,与梯度下降结合构成线性回归算法。
# 19 Machine Learning Specialization [Course 1, Week 1, Lesson 4]

### 梯度下降算法总结

视频总结了线性回归的梯度下降算法,包括 cost 函数 J、模型预测函数 F、参数 W 和 B、梯度下降更新公式。

### 求导数计算

用微积分规则计算 cost 函数 J 对 W 和 B 的导数,即梯度方向。这一步有助于理解算法原理但不是必需的。

### 更新公式

W 和 B 在每步根据导数更新,促使 J 收敛至最小值。更新需同时进行,否则效果可能不同。

### 学习率作用

学习率α控制每步更新幅度,大小影响收敛速度和效果。过小收敛慢,过大可能跳过极值点。

### 开发目标单一

利用 cost 函数为凸函数性质,线性回归的梯度下降在适当α下一定收敛至全局最小值。

### 实例演示

最后一课将通过例子观察算法在数据上的实际运作,进一步加深理解线性回归模型训练过程。
# 20 Machine Learning Specialization [Course 1, Week 1, Lesson 4]

### 梯度下降实例演示

视频中通过实例展示线性回归梯度下降算法的运作过程,包含了模型预测函数、成本函数、参数初始化、每步更新等。

### 学习过程解读

每次更新W和B后,成本函数值和拟合直线均有改变,逐步趋向全局最小值和最佳拟合直线。

### 批梯度下降

求导时计算所有训练实例误差和,每步查看全部数据,称为批梯度下降。

### 预测应用示例

根据训练好的模型可预测房产价格,如某房1250平方尺可能售25万。

### 下一周内容预告

下周将学习线性回归多变量版本,fitting非线性曲线,并给出适用线性回归的实用提示。

### 自测与后续计划

鼓励完成实践题和lab理解算法,并准备在未来自主实现类似算法。下一周课将使模型应用范围更广泛useful。
# 22 Machine Learning Specialization [Course 1, Week 2, Lesson 1]

### 向量化提高代码效率

向量化可以用更简洁的代码实现算法,同时利用并行计算提高运行效率。

### 数值线性代数库

常用的Python数值计算库NumPy支持向量化运算,能利用GPU加速计算。

### 标量与向量定义

视频以W,B,X向量为例说明标量与向量的Python中定义与运算方式。

### for循环实现

使用for循环依次计算每个特徵的权重乘积 soma和,但效率低下。

### 向量运算实现

用NumPy中的dot函数实现W,X向量点积运算,一行代码实现算法,效率大幅提升。

### 向量化优点

向量化不仅简洁,背后利用并行计算能力,无论CPU还是GPU性能都远超循环实现。

### 复杂情况下的应用

当样本量大时,向量化尤为重要。它使代码更易于维护,同时保证效率。
# 23 Machine Learning Specialization [Course 1, Week 2, Lesson 1]

### 向量化效率提升原因

利用并行计算能力,将逐个操作并行化为同时操作数组,大幅提高效率。

### NumPy实现内部机制

NumPy函数利用CPU多核或GPU实现并行计算,如点积一次计算所有数值积。

### 循环与向量化区别

循环逐一操作,向量化数组运算使所有操作同时进行。

### 多特征线性回归示例

以16维特征为例,循环更新需16步,向量化一次更新16个参数。

### 可选实验内容

介绍NumPy基本语法,如创建数组、点积运算,并对比向量化与循环效率。

### 算法应用范围

向量化对特征维数大或数据规模大时性能提升显著。是机器学习算法优化重要手段。

### 下步内容

将多元线性回归与梯度下降结合,实现并行梯度下降算法。
# 24 Machine Learning Specialization [Course 1, Week 2, Lesson 1]

### 多变量线性回归模型

使用向量表示法可以简洁地表示多变量线性回归模型,其中W为参数向量,B为偏置量。

### 预测函数与代价函数

预测函数定义为特征向量X与参数向量W的点积加偏置B。代价函数J以W和B为输入。

### 梯度下降算法

根据导数公式,将每个参数Wj单独更新。更新规则类似单变量情况,但参数和特征都使用向量表示。

### 正常方程方法

除梯度下降外,还有正常方程直接解决W和B。但仅适用于线性回归,速度也随特征数增加而下降。

### Python实现

随视频是可选练习,介绍如何用NumPy定义模型,计算预测、代价及实现梯度下降算法。

### 多变量线性回归应用广泛

它是机器学习中最基础和常用的算法之一。通过选择特征和学习率可有效改进效果。
# 25 Machine Learning Specialization [Course 1, Week 2, Lesson 2]

### 特征比例对模型影响

特征值范围不同会影响参数大小和梯度下降效果。

### 房产预测示例

根据房屋面积和卧室数预测价格,两特征范围不同。

### 参数选择

当特征值大时,对应的参数应小;特征值小时,参数应大。

### 梯度下降曲面分析

不同特征范围导致成本函数椭圆形,影响收敛效率。

### 特征规范化

通过变换,使所有特征值范围一致,如0-1,可以改善参数更新。

### 规范化效果

规范化后成本函数更圆形,梯度下降路径更直接,可提高速度和效率。

### 下一步内容

具体介绍特征规范化实现方式。正确规范化数据对算法至关重要。
# 26 Machine Learning Specialization [Course 1, Week 2, Lesson 2]

### 特征标准化方式

#### dividing by max

以最大值进行除法,将所有特征范围缩减到0-1。

#### mean normalization

计算平均值,将特征整体偏移到0为中心。

#### z-score normalization

计算平均值和标准差,进行标准化处理。

### 标准化效果

标准化后特征值范围一致,梯度下降曲线更圆滑。

### 标准化规则

通常将特征值范围缩放到-1到1范围内较好,但-3到3也可接受。

### 特征值选择标准化

值范围太大(如-100到100)或太小(如0.001)应进行标准化。

### 标准化优点

提升计算速度,且不会对结果产生不利影响,常规进行标准化处理。

### 后续内容

检测梯度下降收敛情况和选择学习率大小。
# 27 Machine Learning Specialization [Course 1, Week 2, Lesson 2]

### 监测梯度下降收敛情况

将训练代价函数J值在各次迭代中的变化绘制为学习曲线图。

### 学习曲线特征

如果梯度下降有效,每次迭代J值将下降;迭代次数增加,J值曲线趋于平缓,表明已经收敛。

### 自动收敛判断

设定ε阈值,如果两次迭代J值下降小于ε,即认为已经收敛。但选择ε较难,建议查看学习曲线。

### 收敛与参数优化

收敛表示找到的参数θ近似全局最佳,可以很好地拟合训练数据。

### 迭代次数不确定

不同问题收敛迭代次数不同,从几十到上万不等,必须实时监测学习曲线才知道。

### 后续内容

根据收敛监测结果,调整学习率α,使梯度下降更快、更稳定地收敛到最优解。
# 28 Machine Learning Specialization [Course 1, Week 2, Lesson 2]

### 学习率的影响

学习率如果太小,梯度下降收敛慢;如果太大,可能导致 cost 函数波动或不收敛。

### 调整学习率

可以通过绘制 cost 曲线来观察不同学习率的效果。学习率应使 cost 函数每次迭代都下降,且下降速度快而稳定。

### 选择学习率

可以尝试 0.001、0.003、0.01 等递增值,选择使 cost 最快下降且稳定的学习率。

### 微调学习率

也可以将学习率设置得非常小,查看是否每次迭代 cost 都下降,来检验梯度下降算法是否正确。

### 学习率影响训练

学习率过小会使训练很慢;过大可能导致 cost 不下降或波动。需平衡学习速度和收敛性能。

### 后续内容

介绍使用自定义特征来拟合多项式曲线模型,而不只是线性模型。
# 29 Machine Learning Specialization [Course 1, Week 2, Lesson 2]

### 特征选择影响学习性能

选择或设计合适的特征对模型效果影响很大,是很重要的一步。

### 案例:预测房产价格

以房产面积、位置等特征预测价格。

### 特征处理

可以利用已有特征(如宽度、深度)计算新特征(如面积),增加预测能力。

### 特征工程

根据问题知识和原始特征,转化或组合新特征,使学习算法更易得出准确预测。

### 房产面积特征设计

以宽度乘以深度得出面积新特征,比单独使用宽度、深度特征效果好。

### 特征工程概念

利用问题知识设计新特征,使学习任务更易,是机器学习重要技术。

### 后续内容

将介绍通过多项式特征设计,学习非线性关系,不仅限于线性模型。
# 30 Machine Learning Specialization [Course 1, Week 2, Lesson 2]

### 多项式回归

可以通过多项式特征工程,学习非线性关系,不限于线性模型。

### 房产面积预测案例

原始线性模型拟合效果差,可尝试二次多项式或三次多项式模型。

### 多项式特征设计

如将原始特征 elevation raised to power 2和3作为新特征。

### 特征规范化重要

多项式特征取值范围不同,容易影响学习效率,须事先规范化。

### 多项式选择

可尝试不同阶数的多项式,或开方函数作为特征,选择效果最好的模型。

### 特征选择工具

后期课程中将介绍如何比较不同特征组合,选择效果优秀模型。

### Scikit-learn简化编码

Python机器学习库Scikit-learn实现线性回归仅需几行代码。
# 31 Machine Learning Specialization [Course 1, Week 3, Lesson 1]

### 本课概述

上周学习线性回归,本周学习分类算法逻辑回归。

### 分类问题示例

邮件分类为垃圾邮件与否,医疗分类为恶性乳腺肿瘤与良性。

### 分类输出值

输出仅为有限种类别,如0或1。

### 二分类问题

输出只有两种类别,如垃圾邮件为1,非垃圾邮件为0。

### 类别表示方法

常用0、1或false、true表示类别,0表示负类,1表示正类。

### 线性回归用于分类

可将预测值映射到0-1范围,但存在问题,如决策边界随数据变化。

### 逻辑回归概述

逻辑回归输出限定在0-1范围内,避免线性回归问题,适用于分类任务。

### 下课实践

体验线性回归分类效果,理解其问题点,驱使学习分类算法。
# 32 Machine Learning Specialization [Course 1, Week 3, Lesson 1]

### 逻辑回归模型简介

逻辑回归是分类任务最常用的算法, outputs分类概率。

### 示例:预测肿瘤分类

输入X为肿瘤大小,输出Y为0/1表示良恶性。

### sigmoid函数

将线性模型输出映射到0-1范围,适用于概率解释。

### 构建逻辑回归模型

1. 线性回归模型计算Z值

2. 将Z传入sigmoid函数计算概率值

### 解释输出概率

输出概率表示Y=1的真实概率,如0.7表示70%可能性为恶性。

### 总结

逻辑回归通过sigmoid函数将线性模型引入非线性,输出分类概率,适用于二分类任务。
# 33 Machine Learning Specialization [Course 1, Week 3, Lesson 1]

### 回顾逻辑回归模型

逻辑回归将线性模型的输出映射到0-1区间,以计算分类的概率。

### 决策边界

决策边界是Logistic回归类别预测的分界线,当f(x)≥0.5预测为1类,否则预测为0类。

### 决策边界的计算

决策边界处Z=0,即w×X+b=0。线性模型决策边界为直线,多项式模型可以拟合更复杂决策边界。

### 例一:直线决策边界

参数设置为W1=1,W2=1,b=-3,决策边界为X1+X2=3。

### 例二:圆形决策边界

将Z设为平方项,参数设置使Z=0表示圆形,决策边界为一个圆。

### 决策边界可视化

通过示例了解Logistic回归如何根据特征学习决策边界,将数据分成两类。

### 后续内容

介绍Logistic回归cost函数与梯度下降,以实现对模型参数的学习训练。
# 34 Machine Learning Specialization [Course 1, Week 3, Lesson 2]

### 回顾logistic回归模型

Logistic回归将线性模型输出映射到0-1区间,以计算分类的概率。

### 回顾损失函数

当标签y=1时,单个样本的损失函数为负对数似然;-log(f(x))

当y=0时,损失函数为:-log(1-f(x))

### 整体成本函数

成本函数J为训练样本损失函数平均值,表达式为:

J(w,b) = -(1/m)Σ[yilogf(x)+(1-yi)log(1-f(x))]

### 总结成本函数

可以将单样本损失函数整合到成本函数中,成为一个关于w,b的函数。

### 优点

该成本函数表面呈凸形,可以利用梯度下降等方法优化,寻找全局最小值。

### 后续内容

介绍利用成本函数梯度下降算法,来学习logistic模型的参数w,b。
# 35 Machine Learning Specialization [Course 1, Week 3, Lesson 2]

### 简化logistic损失函数写法

考虑y只能取0或1,利用此性质给出一个更简单的损失函数表达式。

### 等价性推导

详细论证新增表达式在y=0和y=1情况下,都等价于原来的损失函数形式。

### 定义logistic损失函数

损失函数定义为:L=-ylogf(x)-(1-y)log(1-f(x))

### 计算代价函数J

J为损失函数L在训练样本平均值,定义为:
J(w,b)=-(1/m)Σ[ylogf(x)+(1-y)log(1-f(x))]

### 活动实验推荐

可选实验可视化不同参数下的代价函数变化,帮助理解其对模型影响。

### 最大似然估计原理

该代价函数来源于统计学原理—最大似然估计,具有凸性质。

### 后续内容

将运用梯度下降算法学习logistic模型参数,最小化代价函数J(w,b)。
# 36 Machine Learning Specialization [Course 1, Week 3, Lesson 3]

### 梯度下降算法

利用梯度下降算法求解损失函数J(w,b)关于参数w,b的最小值,即学习模型参数。

### 求导数

分别求J(w,b)对wj和b求导,得到更新规则:

wj:=wj-α∂J/∂wj

b:=b-α∂J/∂b

### 参数更新

使用并行更新规则,先计算右边所有表达式,再同时更新所有的wj和b参数。

### logistic回归与线性回归区别

虽然更新规则形式相同,但logistic回归的f(x)用sigmoid函数定义,线性回归用线性模型定义。

### 特征标准化

标准化特征范围能够加速梯度下降的收敛。

### 代码实现

可选实验给出梯度下降在代码中的计算过程,及训练过程可视化。

### Sklearn库应用

教程最后给出如何利用Sklearn快速训练logistic回归模型。
# 37 Machine Learning Specialization [Course 1, Week 3, Lesson 4]

### 定义过拟合和欠拟合

过拟合指模型在训练数据集上表现很好,但在新的未见数据上表现不好。欠拟合指模型本身复杂程度太低,无法很好拟合训练数据集。

### 线性回归例子

以预测房屋价格为例,太简单的一元线性回归模型欠拟合,高次多项式模型过拟合。二次多项式模型表现最好。

### 逻辑回归例子

将肿瘤类型分为良恶性,不同复杂度模型结果解析。

### 目标

寻找既不欠拟合也不过拟合的模型,即高偏差和高方差都很低的模型。

### 过拟合成因

模型参数过多,试图满足所有训练样例,但对新数据泛化能力差。

### 欠拟合成因

模型参数太少,不能很好地 fitted 训练数据规律。

### 后续内容

将介绍正则化等技术来控制过拟合问题,优化模型表现。
# 38 Machine Learning Specialization [Course 1, Week 3, Lesson 4]

### 定义过拟合和欠拟合

过拟合指模型在训练数据集上表现很好,但在新的未见数据上表现不好。欠拟合指模型本身复杂程度太低,无法很好拟合训练数据集。

### 线性回归例子

以预测房屋价格为例,太简单的一元线性回归模型欠拟合,高次多项式模型过拟合。二次多项式模型表现最好。

### 逻辑回归例子

将肿瘤类型分为良恶性,不同复杂度模型结果解析。

### 目标

寻找既不欠拟合也不过拟合的模型,即高偏差和高方差都很低的模型。

### 过拟合成因

模型参数过多,试图满足所有训练样例,但对新数据泛化能力差。

### 欠拟合成因  

模型参数太少,不能很好地fitted训练数据规律。

### 后续内容

将介绍正则化等技术来控制过拟合问题,优化模型表现。
# 39 Machine Learning Specialization [Course 1, Week 3, Lesson 4]

### 回顾正则化概念

正则化通过增加模型复杂度 penalize项,使参数值保持较小,从而减少过拟合。

### 例子解析

举房价预测高次多项式模型例子,解析不同正则化强度Lambda下的训练结果。

### 正则化成本函数形式

新增正则化部分:Σ(λ/2m)ΣWj^2 , λ调节正则化强度与拟合损失间权衡。

通常不对常数项b施加正则化。

### 线性回归正则化

将正则化概念应用于线性回归,成本函数改为J=MSE+λ/2mΣWj^2。

### 逻辑回归正则化

同样将正则化概念应用于逻辑回归,建立新的正则化成本函数。

### 选择λ值

λ=0报过拟合,λ很大报欠拟合。需选择平衡损失函数与正则化项的λ。

### 后续内容

后续视频将正则线性回归和逻辑回归算法具体实现,利用梯度下降最小化正则化成本函数。
# 40 Machine Learning Specialization [Course 1, Week 3, Lesson 4]

### 正则化线性回归梯度下降算法

给出正则化线性回归cost函数的表达式,以及求导得出的W和B的更新规则。

### 更新规则证明

分别给出W和B的更新规则派生过程,可视推导W的表达式含义。

### W更新含义

W更新过程中会乘以一个略小于1的数,实现W每次迭代都会收缩的效果,这就是正则化起作用的原理。

### 可选内容

详细推导W更新表达式各项项的来历,给出一个更直观的视角来理解正则化在每次迭代中如何使W值收缩。

### Python实现

给出通过Python实现正则化线性回归梯度下降算法的具体代码,以供参考。

### 总结

通过正则化可以很好地减轻过拟合问题,特别是特征多而样本少的情况下。此算法能有效解决许多实际问题。
# 41 Machine Learning Specialization [Course 1, Week 3, Lesson 4]

### 正则化逻辑回归梯度下降算法

给出正则化逻辑回归cost函数表达式,以及求导得到的W和B更新规则。

### W更新规则推导

详细给出W更新规则的推导过程,可以直观理解W每次迭代会收缩的机制。

### 算法原理

正则化项使W值收缩,可以防止过拟合。更新规则与正则线性回归类似,但f(x)用Logistics函数定义。

### Python实现

提供正则化逻辑回归梯度下降算法Python代码,方便参考实现。

### 可选内容实验

最后一个练习实验中,可以通过Lambda参数选择是否启用正则化,观察模型效果。

### 总结

通过本视频理解正则化逻辑回归算法,以及如何实现该算法来减轻过拟合问题。
