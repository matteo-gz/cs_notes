# Deep Learning for Computer Vision (UMich EECS 498-007)
>
> <https://www.youtube.com/playlist?list=PLLhQgjrONLVFP1E7p2jWMMeM2FWUf2Qc7>
>
## 1. Lecture 1: Introduction to Deep Learning for Computer Vision (UMich EECS 498-007)

### 1.1 机器视觉简介

机器视觉是利用计算机模拟和理解人类视觉系统的能力来获取和处理图像数据的科学技术。不同于传统的机器学习方法,深度学习假设具有一定的空间结构,并使用多层神经网络来学习不同级别的表示,从而实现对图像数据的模拟和理解。

### 1.2 深度学习原理

深度学习首先利用卷积神经网络来提取图像特征。卷积神经网络可以学习到不同尺度和位置的特征,从而实现视觉感知。深度学习其次使用全连接神经网络对不同层级的特征进行分类。通过对大量样本进行训练,神经网络可以学习到不同特征的表示能力,从而实现图像分类、目标检测等任务。

### 1.3 深度学习在机器视觉中的应用

深度学习在机器视觉中主要应用于以下任务:

1. 图像分类:鉴别图像中的对象类别,例如识别照片中的动物或植物种类。

2. 对象检测:在图像中定位和识别对象的位置,例如找到图片中的人脸或车辆。

3. 图像分割:将图像分割成不同的区域,例如将人和背景分割开来。

4. 风格迁移:将一张图像的风格迁移到另一张图像上。

5. 超分辨率:生成高分辨率图像。

6. 对象跟踪:跟踪视频中的移动目标。

7. 图像生成:生成新的图像样本。

以上计算机视觉任务均使用了深度学习模型,特别是卷积神经网络,取得了很好的效果。

## 2. Lecture 2: Image Classification (UMich EECS 498-007)

### 2.1 图像分类任务

图像分类是指根据图像内容对图像进行分类,比如将一张图片分类为“猫”或者“狗”。它是计算机视觉领域一个重要且基础的任务。

### 2.2 分类模型

常见的图像分类模型主要有卷积神经网络(CNN)和机器学习模型两类:

- 卷积神经网络通过卷积和池化运算来提取图像的高级特征,从而具有很强的图像分类能力。常见的CNN模型有LeNet、AlexNet、VGG、ResNet等。

- 机器学习模型如SVM通过特征工程提取低级特征,学习分类规则进行分类,但对于复杂的图像数据效果不如CNN。

### 2.3 分类模型训练

训练分类模型主要过程包括:

1. 收集带标签的图像数据集进行训练和测试。

2. 定义CNN模型结构,选择优化器和损失函数。

3. 对模型进行前向传播计算分类结果,反向传播计算梯度进行参数更新。

4. 使用测试集评估分类效果,根据结果调整模型参数和结构。

5. 重复训练更新参数,直到模型在测试集上分类效果满意。

### 2.4 常见应用

图像分类技术广泛应用于生活场景,如手机相册自动分类、网络图片搜索、医学图像识别等。随着深度学习的发展,分类效果不断提升。

## 3. Lecture 3: Linear Classifiers (UMich EECS 498-007)

### 3.1 线性分类器原理

线性分类器通过一个线性函数对样本进行分类。该线性函数通常形式是:

f(x) = w^T x + b

其中w是权重矢量,b是偏置项。样本点x如果f(x)大于0,则分类为第一类;否则分类为第二类。

### 3.2 逻辑回归

逻辑回归是一种广泛使用的线性分类器。它通过应用sigmoid函数将线性函数的值映射到0-1区间,以预测样本的概率分布。

其Loss函数为交叉熵,通常使用梯度下降法进行优化。逻辑回归适用于二分类问题。

### 3.3 支持向量机SVM

SVM通过求最大间隔超平面来进行分类。其目的是求解能够最好区分两类点的分割超平面。

其Loss函数为软间隔损失函数,通常使用SMO算法进行优化。SVM可以扩展到多分类问题。

### 3.4 softmax分类器

Softmax分类器即多类逻辑回归,用于多分类问题。它通过计算每个类别的似然概率进行分类。

其Loss函数也为交叉熵。Softmax分类器可以用于深度学习最后的分类层。

以上线性分类器在深度学习模型的架构中广泛使用,提供初始级别的分类能力。

## 4. Lecture 4: Optimization (UMich EECS 498-007)

### 4.1 优化目标

深度学习模型训练的目标是通过优化模型的参数,来最小化损失函数值。损失函数衡量模型预测值和真实值之间的差异。

### 4.2 优化算法

常用的深度学习优化算法包括:

- 梯度下降法:通过计算参数对损失函数的梯度,来更新参数以最小化损失值。

- 随机梯度下降:每一步仅使用一个样本来更新参数,从而缩短训练时间。

- 最小二乘法:特别针对平方损失函数进行优化。

- 优化器:如Momentum、AdaGrad、RMSProp、Adam等可加速和稳定梯度下降。

### 4.3 实现步骤

深度学习优化主要步骤:

1. 计算前向传播得到预测值。

2. 计算损失函数获得代价值。

3. 通过反向传播计算各参数对损失函数的梯度。

4. 使用优化算法更新参数,以减小损失函数。

5. 不断重复上述过程,逼近全局最优解。

### 4.4 学习率策略

学习率可以改变权重更新的幅度,需要动态调整来优化学习过程。常用策略有固定率、衰减率、指数衰减等。

## 5. Lecture 5: Neural Networks (UMich EECS 498-007)

### 5.1 神经网络基本原理

神经网络模拟人脑神经网络,由连接的节点单元构成。每个节点都有一个激活值,相邻节点间通过加权连接。神经网络的基本功能单元是人造神经元,也称为感知器。

### 5.2 多层感知器

多层感知器是最简单的神经网络结构,包含输入层、隐藏层和输出层。感知器通过线性组合输入,后经非线性激活函数处理输出。

### 5.3 误差反向传播

使用误差反向传播算法训练多层感知器,即采用链式法则计算隐藏层关于损失函数的梯度。以最小化整体损失为目标,反向传播梯度来更新每个参数。

### 5.4 激活函数

常用的激活函数包括阶跃函数、sigmoid、tanh、ReLU等。它们通过非线性转换,使神经网络能学习非线性和高级特征。

### 5.5 神经网络训练

使用梯度下降法来优化多层感知器,主要步骤为:向前计算、计算损失、反向传播错误信号、更新参数。通过不断迭代来最小化整体损失函数。

## 6. Lecture 6 Backpropagation (UMich EECS 498-007)

### 6.1 误差反向传播原理

误差反向传播算法通过链式法则,来计算每个神经元参数对最后损失函数的导数。它帮助神经网络在训练样本上最小化代价函数。

### 6.2 前向传播

前向传播是神经网络运行的正向过程:输入被传递并反复计算,直到计算输出。各层神经元激活尚未保存。

### 6.3 后向传播

后向传播首先计算输出层到损失函数的导数,然后使用链式法则逆向计算各层的参数导数。将导数保存用于参数更新。

### 6.4 参数更新

根据损失函数对各参数的导数,使用梯度下降法更新参数。更新参数的值为旧值减去学习率乘以导数。

### 6.5 训练过程

遍历整个训练数据集,重复前向传播、后向传播和参数更新过程,不断调整网络参数以最小化损失函数。经过多轮迭代训练后收敛到最优解。

误差反向传播算法将深度学习神经网络的训练过程规范化,并使其在大规模数据集上高效运行。

## 7. Lecture 7: Convolutional Networks (UMich EECS 498-007)

### 7.1 卷积神经网络简介

卷积神经网络(CNN)利用卷积操作来提取图片中的语义特征,在计算机视觉中表现突出。

### 7.2 卷积层

卷积层采用小过滤器 sliding遍全图,提取各个区域的特征。卷积核可以学习到不同方向与范围的特征。

### 7.3 池化层

池化层通过最大值池化或平均池化降低尺寸,增加特征不变性。它可以减少参数,控制过拟合。

### 7.4 形式化卷积层

形式化卷积层模型化卷积和池化为矩阵运算,用多个通道保存特征,体现空间结构特征。

### 7.5 VGG网络

VGG网络通过深层卷积中的小过滤器提取高级特征,增加网络深度。网络表现出色但参数多。

### 7.6 ResNet网络

ResNet采用残差结构有效回传梯度,可以构建更深网络,抑制梯度消失问题,极大提升性能。

### 7.7 MobileNet

MobileNet为移动端设计,采用深度可分离卷积,大幅减少参数量,实现设备上的快速推理。

卷积神经网络充分融合空间结构,在计算机视觉许多任务上表现出色。

## 8. Lecture 8: CNN Architectures (UMich EECS 498-007)

### 8.1 VGG网络

VGG网络采用相对小的过滤器如3x3 repeat stack,能学习更复杂的函数映射。网络很深但参数量大。

### 8.2 Inception网络

Inception模块同时使用多尺寸过滤器,可包含1x1、3x3、5x5卷积。通过1x1卷积下采样输入通道,降低计算成本。

### 8.3 ResNet网络

ResNet插入残差连接来跳跃函数,解决梯度弥散问题。使得网络更深更强大。Identity块和卷积块是主要结构。

### 8.4 DenseNet网络

DenseNet每个层都直接连接所有前层,同时传递特征图。更充分挖掘特征表现能力。

### 8.5 NasNet网络

NasNet通过神经体系结构搜索自动设计结构,发挥CNN潜在能力。表现强于手工设计网络。

### 8.6 EfficientNet

EfficientNet通过操纵网络depth、width、resolution三个维度,取得更高精度与效率。

以上各类卷积神经网络架构均发挥CNN各种优点,在目标检测、图像分割等任务有很好表现。

## 9. Lecture 9: Hardware and Software (UMich EECS 498-007)

### 9.1 GPU

GPU并行计算能力强,适合神经网络运算。相比CPU,GPU支持更多并行计算单元,加速深度学习训练。

### 9.2 TPU

TPU是谷歌自研的特定增强型 Chips,专注于神经网络计算。并行程度超过GPU,安全可靠,用于产品级深度学习服务。

### 9.3 主流框架

深度学习框架如TensorFlow、PyTorch、 paddlePaddle等提供易用接口和并行计算能力,用于构建和训练各类模型。

### 9.4 分布式训练

通过在多个GPU/TPU设备和机器上分布训练数据,实现大幅度加速。框架支持数据Parallel与模型Parallel模式。

### 9.5 模型压缩

量化、剪枝、迁移学习等技术可以有效压缩深度学习模型, implementations推断速度和存储成本。

### 9.6 Edge设备

专业的边缘智能设备和移动App平台支持部署量子化模型进行高效推理,适应各种IoT和移动应用场景。

硬件和软件互相促进,共同提升深度学习的计算性能和应用规模,充分发挥其潜力。

## 10. Lecture 10: Training Neural Networks Part 1 (UMich EECS 498-007)

### 10.1 参数初始化

目标是减小梯度消失或爆炸问题。常见初始化方法有零初始化、均匀分布初始化、标准正态分布初始化等。

### 10.2 损失函数

类别问题通常使用交叉熵函数,回归问题使用MSE误差函数。模型训练目标是最小化整体损失。

### 10.3 优化算法

SGD、Momentum、RMSProp、Adam等算法可以加速并稳定训练过程。其中Adam算法表现较好,现已广泛使用。

### 10.4 学习率策略

动态学习率可以帮助训练更稳定。例如设置初始高学习率后逐步衰减,或根据批次波动学习率。

### 10.5 激活函数

ReLU等非线性激活函数可以增加模型表达力。批标准化可以加速训练并改善性能。

### 10.6 正规化

采用权重衰减削弱模型复杂度,可以防止过拟合。丢弃法随机Mask部分神经连接来提升泛化能力。

正确设置上述多个参数和策略是训练有效模型的重要一环。

## 11. Lecture 11: Training Neural Networks Part 2 (UMich EECS 498-007)

### 11.1 监督训练

需要大量带有标签的数据来迭代训练模型参数,最小化训练和验证集上的损失函数。

### 11.2 无监督训练

通过无监督学习任务如种类聚类或维度降低来训练模型表示学习功能。

### 11.3 transfer learning

固定预训练模型低层特征,调整高层分类器来新的任务上,可以用较少数据取得较好效果。

### 11.4 和增强学习

通过对抗例如噪声来扩展数据并有效训练模型分布区域,增强泛化能力。

### 11.5 模型系列化

针对不同任务、设备系列训练出一系列模型 parameters进行选择,精简模型提高性能。

### 11.6深度学习系统

系统设计如分布式架构可以有效利用硬件资源,支持大规模实时学习任务。

### 11.7 当前挑战

难解决的问题包括唯一解没有保证、解释性差等,还有待深度学习在这方面作进一步研究。

正确的训练策略对深度学习模型质量影响重大。

## 12. Lecture 12: Recurrent Neural Networks (UMich EECS 498-007)

### 12.1 循环神经网络

RNN能处理带有顺序关系的数据,通过循环结构有效学习序列间依赖关系。

### 12.2 基本 RNNcell

单个 RNNcell更新其隐藏状态,输出取决于输入和隐藏状态。多层 Cell构成完整RNN。

### 12.3 长短期记忆网络

LSTM引入遗忘门、输入门和输出门来减轻梯度消失问题,更好学习长期依赖。

### 12.4 门控循环单元 GRU

GRU结合了LSTM的三个门机制,使用重置门和更新门简化计算和参数数量。

### 12.5 RNN的应用

语言模型、翻译、语音识别都利用RNN优势,处理序列数据和上下文信息。

### 12.6 训练RNN

使用BPTT回溯算法计算时间步数为T的RNN的梯度。采用隐藏状态作为网络参数。

### 12.7 近年发展

ATTN机制改进翻译质量。Transformer全用ATTN提供迭代建模能力。

RNN系列网络适用于处理顺序数据,在NLP、语音领域表现活跃。

## 13. Lecture 13: Attention (UMich EECS 498-007)

### 13.1 注意力机制

注意力机制可以有效捕捉序列中的关键位置,动态学习上下文相关子集。

### 13.2 周向注意力

以解码器每个位置作为query,与编码器所有位置做内积,得到相关程度分布。

### 13.3 自注意力

自注意力计算query与所有其他位置的关系,赋予序列中的不同部分不同重要程度。

### 13.4 多头注意力

多头注意力将注意力映射分解为不同子空间,得出检索结果的线性组合。

### 13.5 位置编码

为捕捉顺序信息,嵌入位置索引到模型输入。可用绝对位置编码或相对位置编码。

### 13.6 Transformer

Transformer模型完全采用注意力机制,消除递归结构,加速训练与推理。取得语言处理新突破。

### 13.7 Attention应用

图像caption、机器翻译等领域都大量采用注意力机制。是深度学习新的重要研究方向。

注意力机制对捕获序列全局依赖关系有很好的建模能力。

## 14. Guest Lecture: Adversarial Machine Learning (UMich EECS 498-007)

### 14.1 对抗例子

通过对输入进行不易察觉的扰动,可以引导模型给出错误预测。这是模型鲁棒性的重要问题。

### 14.2 攻击算法

如Fast Gradient Sign Method(FGSM)使用输入梯度方向进行对抗扰动。可实现高成功率攻击。

### 14.3 防御算法

例如微小扰动方式训练模型学习不变性,增加噪声提升模型鲁棒性。预测时进行预处理也能增加security。

### 14.4 对抗训练

在训练时增加对抗例子可以强化模型,使其对适当大小扰动免疫。但可能影响标准例子的表现。

### 14.5 隐私保护

隐式反向传播法可以隐藏训练数据的细节,使对抗者难以从模型推断 Examples。

### 14.6 其他思路

Contractive交叉熵、迁移学习也是缓解对抗攻击的有效方式。需要在公平、安全、隐私方面进行综合考量。

对抗学习是深度学习安全与解释能力的重要研究方向。

## 15. Lecture 14: Visualizing and Understanding (UMich EECS 498-007)

### 15.1 视觉化技巧

使用Grad-CAM等方法可视化神经网络关注的图像区域,了解其学习策略。

### 15.2 样例复杂度

根据样例困难值衡量其难易程度,鉴别边缘和棘手样本。

### 15.3 嵌入空间可视化

TSNE等技术用于降维观察高维样本特征分布结构,发现数据分组情况。

### 15.4 理解传播

回溯损失函数对输入的梯度影响,找出决定因素。冷熔断法发现模型抽象性。

### 15.5 可解释模型

线性模型、树模型结果更易理解。对抗训练增强模型稳定性和可解释性。

### 15.6 模型提问

让模型进行自我质疑,回答其预测依据,有助于发现盲点和错误逻辑。

### 15.7 人工社区

依靠人工标注提升理解能力,实现自动学习和知识传承。

深度学习解释和可视化是保障其安全可靠性的重要一环。

## 16. Lecture 15: Object Detection (UMich EECS 498-007)

### 16.1 检测任务

检测需同时预测每个目标的类别和位置坐标框。相比分类,难度更大。

### 16.2 区域 proposta网

先建议可能含目标的区域proposals,再利用CNN进行分类和校正回归框位置。

### 16.3 SSD手法

在特征图上直接预测目标位置和类别,简化步骤。支持实时检测。

### 16.4 Faster R-CNN

提出RPN网络提取proposal,后接设计好的检测网络进行检测。精度高速度快。

### 16.5 Mask R-CNN

在Faster R-CNN基础上增加一个分支进行目标分割掩码预测。

### 16.6 YOLO

直接从整个图片预测位置和类别,低延迟但准确度限制。

### 16.7 目标检测Benchmark

COCO提供标准测试集和评价指标,如AP支持跨数据集验证算法效果。

物体检测是计算机视觉的基础任务,也广泛应用于嵌入式领域。

## 17. Lecture 16: Detection and Segmentation (UMich EECS 498-007)

### 17.1 实例分割

与检测任务相比,要同时给每个目标预测一个实例掩码,将其与背景正确分割开。

### 17.2 Mask R-CNN

在Faster R-CNN架构下增加一个分支进行掩码预测,取得优异的实例分割效果。

### 17.3 Panoptic分割

目标是将图像完整分割为具体语义部分和背景,兼顾实例和语义分割。

### 17.4 Deeplab系列

采用空间扩充操作和注意力机制获取全局上下文,优化语义分割效果。

### 17.5 光流

通过追踪像素点跨帧运动,可以实现视频中的物体实例分割与跟踪任务。

### 17.6 医学影像分割

将CT、MRI等医疗影像准确分割为脏器、损伤部位,提高诊断效果。

### 17.7 其他应用

自然语言理解、细胞器自动标注等任务也利用实例和语义分割技术。

分割网络在各个领域都有重要应用,发展迅速。

## 18. Lecture 17: 3D Vision (UMich EECS 498-007)

### 18.1 3D重建

从一系列2D图像内外参推断场景3D几何结构的过程。

### 18.2 立体匹配

利用视差信息计算两幅图像对应点的距离差异,用于三维重建。

### 18.3 PointNet

直接对三维点云进行处理,实现点云分类和部分识别任务。

### 18.4 Voxel网

将3D空间量化为三维体素,学习既定空间内的特征向量表示。

### 18.5 应用

3D重建用于增强现实、自动驾驶等。医疗领域三维重建也很有价值。

### 18.6 3D神经网络

采用空间卷积等操作处理体素特征,实现3D场景语义理解任务。

### 18.7 动态重建

运用光流等技术对时间序列图像进行3D重建,可实现视频重建等。

三维计算机视觉面临挑战,但在许多领域都有重要应用潜力。

## 19. Lecture 18: Videos (UMich EECS 498-007)

### 19.1 视频理解任务

包括行为识别、视频分类、视频生成等。难点在于建模时空依赖关系。

### 19.2 RNN处理视频

每个帧作为时间步输入RNN处理视频序列。LSTM等解决梯度消失问题。

### 19.3 3D卷积神经网络

将视频看作3D格式的数据,利用3D卷积神经网络进行视频特征提取。

### 19.4 时空注意力

注意力机制有效捕捉视频各帧间的时空关系及关键帧。

### 19.5 房间语义理解

通过多视角相机捕捉房间行为序列,实现深度理解。

### 19.6 生成对抗网络

对抗训练视频GAN生成逼真操作样本供下游任务使用。

### 19.7 低帧率视频

探索利用少量帧就可以完成视频任务,例如识别和生成。

视频理解是开放领域,模型需捕获丰富的时空依赖关系。

## 20. Lecture 19: Generative Models Part 1 (UMich EECS 498-007)

### 20.1 生成模型概述

生成模型学习数据分布,能对新样本进行可不同形式的生成。

### 20.2 VAE(变分自动编码器)

VAE将样本映射到低维浮点向量空间,再重构原样本,支持任务如插值等。

### 20.3 自编码器

自编码器通过编码-解码学习 destroying和重构输入,用于预训练深度网络。

### 20.4 邻近编码

将样本编码为其邻近原型矢量的加权组合,支持实例修补等下游任务。

### 20.5 对抗例子

GAN通过对抗训练生成器产生类似真实样本的假样本,难以收敛但生成效果好。

### 20.6 评估生成模型

可视化输出样本质量,或采取抽样后标签检测评价生成效果复杂度。

生成模型是未来人工智能一个重要方向。

## 21. Lecture 20: Generative Models Part 2 (UMich EECS 498-007)

### 21.1 GAN算法

通过生成器产生假样本,鉴别器检测真伪,反馈让生成器不断优化,实现对抗训练。

### 21.2条件GAN

给生成器输入额外条件标签,能生成对应标签的假样本,如图片文本GAN。

### 21.3深度生成对抗网络

多层神经网络结构深化GAN,实现更好生成效果。

### 21.4CycleGAN

采用循环一致性损失函数在不同域之间实现无监督图片转换效果好。

### 21.5实用GAN

通过改进训练方法,生成更逼真高分辨率样本用于下游任务。

### 21.6文本到图片

采用注意力GAN结构生成与文本描述恰当的图象,很具挑战性。

### 21.7未来展望

全面理解GAN训练难点和更深度结合生成模型与其他领域等。

生成对抗网络在图像合成、风格迁移等方面应用广泛。

## 22. Lecture 21: Reinforcement Learning (UMich EECS 498-007)

### 22.1 强化学习概述

定义状态、动作、奖励机制,训练智能体通过与环境交互获取最大累积奖励。

### 22.2 Markov决策过程

使用马尔可夫性质定义问题,求解最优策略π使长期奖励最大。

### 22.3 Q学习

估计状态-动作值函数Q(s,a),选择最大Q值动作,简化学习空间。

### 22.4 策略梯度

直接优化 policy πθ(a|s),利用REINFORCE算法计算策略梯度进行更新。

### 22.5 Deep Q Network

采用深度神经网络拟合Q函数,与经验回放一起有效解决强化学习问题。

### 22.6 AlphaGo Zero

只通过自我对弈学习,成功挑战围棋大师,突破人类难以企及的水平。

### 22.7 应用

强化学习在游戏、机器人控制、自动驾驶等取得突破性进展。

强化学习是AI领域重要研究方向之一。

## 23. Lecture 22: Recap & Open Problems (UMich EECS 498-007)

### 23.1 课程回顾

训练模型、优化算法、卷积神经网络、递归神经网络、注意力机制、生成模型、强化学习等。

### 23.2 计算能力极限

硬件是否可以支持更大深度和规模的模型训练与应用?

### 23.3 解释性

如何解释深度学习模型内部工作机制和个案预测结果?

### 23.4 隐私与公平

如何保护个人隐私并实现模型预测公平性?

### 23.5 六月问题

人工智能是否有可能发展超出人类控制?如何规避此类风险?

### 23.6 强化学习

如何在更复杂动态环境中有效应用强化学习?

### 23.7 通用人工智能

能否设计一种通用框架实现不同任务的智能体?

深度学习仍面临许多挑战需要全球研究者共同努力解决。
